{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1장. 다중 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 목표\n",
    "- 다중 선형 회귀의 최적화 방식을 학습합니다. \n",
    "- 다중 선형 회귀 class를 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 다중 선형 회귀 모델\n",
    "1. 다중 선형 회귀란?\n",
    "\n",
    "### 2. 다중 선형 회귀 최적화\n",
    "1. 다중 선형 회귀 least square solution\n",
    "2. 다중 선형 회귀 gradient descent\n",
    "\n",
    "### 3. 다중 선형 회귀 함수 class \n",
    "1. 단중 선형 회귀 class 구조\n",
    "2. 함수 정의\n",
    "3. scikit-learn 모듈\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  다중 선형 회귀 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 다중 선형 회귀 least square solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단순 선형 회귀때와 같은 진행 방식으로 최적화 과정을 거치기 전에 앞서 필요한 요소들을 정리해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 회귀 모델에 대해서 설명하기 앞서 학습 데이터가 어떤 형태를 갖고 있는지 알아봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 학습용 feature 데이터\n",
    "> $$X_{feature}=\\begin{pmatrix}\n",
    "\\mathbf{x_1}^T \\\\ \n",
    "\\mathbf{x_2}^T \\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf{x_N}^T\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    " x_{1,1} & ... & x_{1,p} \\\\ \n",
    " x_{2,1} & ... & x_{2,p} \\\\ \n",
    " \\vdots  & \\ddots  & \\vdots \\\\ \n",
    " x_{N,1} & ... & x_{N,p}\n",
    "\\end{pmatrix}, \\;\\;\\;\\;\\; $$\n",
    "\n",
    "##### 학습용 lable 데이터\n",
    "> $$\\mathbf{y}=\\begin{pmatrix}\n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "\\vdots \\\\ \n",
    "y_N\n",
    "\\end{pmatrix}. \\;\\;\\;\\;\\; $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 선형 회귀이기에 2개 이상인 $p$개의 feature column을 갖는 데이터를 사용하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다중 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번엔 다중 선형 회귀 모델에 대해서 정의해 봅시다.\n",
    "\n",
    "다중 선형 회귀 모델은 $p$개의 feature column을 입력으로 하여 1차 함수 형태로 회귀 모델을 구현한 것을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 다중 선형 회귀 모델  f()\n",
    "> $$f(\\mathbf{x_i})=w_0+w_1 x_{i,1}+w_2 x_{i,2}+...+w_p x_{i,p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_0, w_1, w_2,...,w_p$은 1차 함수 모델 $f()$의 파라미터를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반화 된 형태의 least square solution을 구하기 위해서 보다 효율적으로 다중 선형 회귀 모델을 표현할 수 있는 행렬식으로 정리해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 다중 선형 회귀 모델 F()\n",
    "\n",
    "> $$F(X) = X\\mathbf{w}$$\n",
    "\n",
    "> $$X=\\begin{pmatrix}\n",
    "1 & x_{1,1} & ... & x_{1,p} \\\\ \n",
    "1 & x_{2,1} & ... & x_{2,p} \\\\ \n",
    "1 & \\vdots  & \\ddots  & \\vdots \\\\ \n",
    "1 & x_{N,1} & ... & x_{N,p}\n",
    "\\end{pmatrix}, \\;\\;\\;\\;\\; \\mathbf{w}=\\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\ \n",
    "w_2 \\\\ \n",
    "\\vdots \\\\ \n",
    "w_p\n",
    "\\end{pmatrix}, \\;\\;\\;\\;\\; $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$는 기존 feature 데이터에서 1만으로 이루어진 column을 추가한 형태를 의미합니다. \n",
    "\n",
    "$\\mathbf{w}$는 파라미터로 이루어진 벡터를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 1> 다중 선형 회귀 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 선형 회귀 모델을 만들어 2개 이상의 column 배열을 갖는 feature 데이터를 입력하고 예측값을 출력하여 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_feature: \n",
      "[[1 3]\n",
      " [2 6]\n",
      " [3 7]\n",
      " [4 8]]\n",
      "\n",
      "X: \n",
      "[[1. 1. 3.]\n",
      " [1. 2. 6.]\n",
      " [1. 3. 7.]\n",
      " [1. 4. 8.]]\n",
      "\n",
      "Y: \n",
      "[[3.1]\n",
      " [4.9]\n",
      " [7.2]\n",
      " [8.9]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# feature 데이터\n",
    "X_feature = np.array([[1, 2, 3, 4],[3, 6, 7, 8]]).transpose()\n",
    "# 1 column 이 추가된 feature 데이터\n",
    "X = np.c_[np.ones((X_feature.shape[0],1)),X_feature]\n",
    "# label 데이터\n",
    "Y = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))\n",
    "\n",
    "print(\"X_feature: \\n{}\\n\".format(X_feature))\n",
    "print(\"X: \\n{}\\n\".format(X))\n",
    "print(\"Y: \\n{}\\n\".format(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F(X): \n",
      "[[ 5.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [13.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 column 설정\n",
    "w = np.array([1,1,1]).reshape((-1,1))\n",
    "\n",
    "# 다중 선형 모델 함수\n",
    "def F_X(w,X):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "print(\"F(X): \\n{}\\n\".format(F_X(w,X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss 함수 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 선형 회귀 모델을 정리 했으니 마지막으로 loss 함수를 정리해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss 함수\n",
    "\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "Loss(\\mathbf{w})&=\\frac{1}{N}\\sum_{i=1}^{N}(y_i-f(\\mathbf{x_i}))^{2} \\\\\n",
    "&=\\frac{1}{N}(\\mathbf{y}-X\\mathbf{w})^{T}(\\mathbf{y}-X\\mathbf{w})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행렬 수식이 등장해 다소 헷갈릴 수 있지만 이것을 풀어보면 결국 같다는 것을 확인 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 오차\n",
    "\n",
    "> $$(\\mathbf{y}-X\\mathbf{w})=\\begin{pmatrix}\n",
    "y_1-(w_0 + w_1 x_{1,1} + .... + w_p x_{1,p}) \\\\ \n",
    "y_2-(w_0 + w_1 x_{2,1} + .... + w_p x_{2,p}) \\\\ \n",
    "\\vdots \\\\ \n",
    "y_p-(w_0 + w_1 x_{p,1} + .... + w_p x_{p,p})\n",
    "\\end{pmatrix}. \\;\\;\\;\\;\\; $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 least square solution을 구하게 되면 다음과 같이 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### least sqaure solution\n",
    "\n",
    "> $$\\mathbf{\\widehat{w}}=(X^{T}X)^{-1}X^{T}\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\widehat{w}}$은 loss 값을 최소로 만드는 파라미터 벡터를 의미합니다.\n",
    "\n",
    "이에 대한 유도 과정은 뒤의 심화 과정에서 설명하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 2> 다중 선형 회귀 구현 - least square "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 선형 회귀 최적의 파라미터를 least square로 구하고 최소화 된 loss 값을 출력해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.014999999999999982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# feature 데이터\n",
    "X_feature = np.array([[1, 2, 3, 4],[3, 6, 7, 8]]).transpose()\n",
    "# 1 column 이 추가된 feature 데이터\n",
    "X = np.c_[np.ones((X_feature.shape[0],1)),X_feature]\n",
    "# label 데이터\n",
    "Y = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))\n",
    "\n",
    "\n",
    "w = np.dot(np.linalg.inv(np.dot(X.transpose(),X)),np.dot(X.transpose(),Y))\n",
    "\n",
    "# 다중 선형 모델 함수\n",
    "def F_X(w,X):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "\n",
    "def loss(f_x, label_data):\n",
    "    error = label_data - f_x\n",
    "    ls = np.mean(error**2)\n",
    "    return ls\n",
    "\n",
    "print(\"loss: {}\\n\".format(loss(F_X(w,X),Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**심화 학습 - 다중 선형 회귀 least square 유도**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단순 선형 회귀 때와 마찬가지로 파라미터의 이차 함수 형태로 정의된 loss 함수는 최소인 지점에서 기울기가 0입니다. \n",
    "\n",
    "단순 선형 회귀에서는 이러한 이차 함수의 특성을 이용하여 단 2개의 파리미터에 대한 편미분 구하였습니다.\n",
    "\n",
    "하지만 다중 선형 회귀에서는 $p+1$ 개의 파라미터가 존재하기에 이 모든 것에 대해서 편미분을 하기는 쉽지 않습니다.\n",
    "\n",
    "따라서 벡터 형태인 파라미터로 미분을 하는 방식을 사용하여 편미분을 수행합니다.\n",
    "\n",
    "미분을 하기 위하여 loss 함수의 행렬 수식을 다시 정리를 하자면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss 함수 - 행렬을 사용한 표현\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "Loss(\\mathbf{w})&=\\frac{1}{N}(\\mathbf{y}^{T}-\\mathbf{w}^{T}X^{T})(\\mathbf{y}-X\\mathbf{w}) \\\\\n",
    "&=\\frac{1}{N}(\\mathbf{y}^{T}\\mathbf{y} -\\mathbf{y}^{T}X\\mathbf{w} - \\mathbf{w}^{T}X^{T}\\mathbf{y} + \\mathbf{w}^{T}X^{T}X\\mathbf{w}) \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 loss 함수를 $\\mathbf{w}$ 을 사용하여 편미분하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss 함수 편미분\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "\\frac{\\partial Loss}{\\partial \\mathbf{w}}&=\\frac{1}{N}(\\frac{\\partial \\mathbf{y}^{T}\\mathbf{y}}{\\partial \\mathbf{w}} - \\frac{\\partial \\mathbf{y}^{T}X\\mathbf{w}}{\\partial \\mathbf{w}} - \\frac{\\partial \\mathbf{w}^{T}X^{T}\\mathbf{y}}{\\partial \\mathbf{w}} + \\frac{\\partial \\mathbf{w}^{T}X^{T}X\\mathbf{w}}{\\partial \\mathbf{w}})\\\\\n",
    "&=\\frac{1}{N}(0-\\mathbf{y}^{T}X-(X^{T}\\mathbf{y})^{T}+2\\mathbf{w}^{T}X^{T}X)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기가 0인 지점을 찾기 위하여 편미분 한 것이므로 다음과 같은 방정식을 만족하는 $\\mathbf{w}$는 $\\mathbf{\\widehat{w}}$입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 최적의 w 값을 찾는 과정\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "\\frac{\\partial Loss}{\\partial \\mathbf{\\widehat{w}}}&=0\\\\\n",
    "\\frac{1}{N}(0-\\mathbf{y}^{T}X-(X^{T}\\mathbf{y})^{T}+2\\mathbf{\\widehat{w}}^{T}X^{T}X)&=0\\\\\n",
    "\\mathbf{\\widehat{w}}^{T}X^{T}X &= \\mathbf{y}^{T}X\\\\\n",
    "X^{T}X\\mathbf{\\widehat{w}} &= X\\mathbf{y}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $X^{T}X$의 역행렬이 존재한다 가정한다면, $\\widehat{w}$는 다음과 같이 정리할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 최적의 해\n",
    "\n",
    "> $$\\mathbf{\\widehat{w}}=(X^{T}X)^{-1}X^{T}\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역행렬의 존재 조건은 $X$의 rank 값이 full rank인 경우이기에 feature column 벡터 간의 선형 독립을 만족해야하 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가로 least square solution 유도에 필요한 벡터 미분에 대해서 아래와 같이 정리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 행렬 크기 정리\n",
    "\n",
    "> $$\\mathbf{w} : mx1 \\; matrix$$\n",
    "\n",
    "> $$X: Nxm \\;matrix$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 벡터 미분 정리\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "y=X \\;\\;\\;\\;\\;\\;\\;\\;\\;&-> \\frac{\\partial y}{\\partial \\mathbf{w}}=0\\\\\n",
    "y=X\\mathbf{w}\\;\\;\\;\\;\\;\\;\\; &-> \\frac{\\partial y}{\\partial \\mathbf{w}}=X\\\\\n",
    "y=\\mathbf{w}^{T}X^{T} \\;\\;\\; &-> \\frac{\\partial y}{\\partial \\mathbf{w}}=X^{T}\\\\\n",
    "y=\\mathbf{w}^{T}X^{T}\\mathbf{w}\\; &-> \\frac{\\partial y}{\\partial \\mathbf{w}}=2\\mathbf{w}^{T}X^{T}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터 미분에 대한 더 자세한 설명은 다음 링크를 참고해서 학습해 봅시다. https://en.wikipedia.org/wiki/Matrix_calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
