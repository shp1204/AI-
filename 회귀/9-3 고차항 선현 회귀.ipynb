{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2장. 다항 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 목표\n",
    "- 다항 선형 회귀 모델의 구조와 고차항이 추가될수록 변화하는 특성을 학습합니다. \n",
    "- 다항 선형 회귀 class를 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 다항 선형 회귀 모델\n",
    "1. 다항 선형 회귀란?\n",
    "\n",
    "### 2. 이차항 선형 회귀\n",
    "1. 이차항 선형 회귀 모델\n",
    "2. 이차항 선형 회귀 class 구조\n",
    "3. scikit-learn 모듈\n",
    "\n",
    "### 3. 고차항 선형 회귀\n",
    "1. 고차항 선형 회귀 모델\n",
    "2. 고차항 선형 회귀 class 구조\n",
    "3. scikit-learn 모듈\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  고차항 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. 고차항 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이차항 선형 회귀 모델을 잘 이해했다면 고차항 선형 회귀는 그 확장형이라 생각할 수 있습니다.\n",
    "\n",
    "이차항 선형 회귀 모델에서와 같이 데이터부터 정의해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 학습용 feature 데이터\n",
    "\n",
    "> $$X_{feature}=\\begin{pmatrix}\n",
    "1 & x_{1,1} & ... & x_{1,p} \\\\ \n",
    "1 & x_{2,1} & ... & x_{2,p} \\\\ \n",
    "1 & \\vdots  & \\ddots  & \\vdots \\\\ \n",
    "1 & x_{N,1} & ... & x_{N,p}\n",
    "\\end{pmatrix}, \\;\\;\\;$$\n",
    "\n",
    "##### 학습용 lable 데이터\n",
    "\n",
    "> $$\\mathbf{y}=\\begin{pmatrix}\n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "\\vdots \\\\ \n",
    "y_N\n",
    "\\end{pmatrix}. \\;\\;\\;\\;\\; $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 선형 회귀와 마찬가지로 $p$개의 feature column을 갖는 feature 데이터로 일반화하여 학습데이터를 정의합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 고차항 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 고차항 선형 모델에 대해서 정의해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 고차항 선형 회귀 모델\n",
    "\n",
    "> $$F(X) = X\\mathbf{w}$$\n",
    "\n",
    "> $$X=\\begin{pmatrix}\n",
    "1 & x_{1,1} & \\cdots & x_{1,1}^{q} & x_{1,2} & \\cdots & x_{1,2}^{q} & \\cdots & x_{1,p}^{q} \\\\ \n",
    "1 & x_{2,1} & \\cdots & x_{2,1}^{q} & x_{2,2} & \\cdots & x_{2,2}^{2} & \\cdots & x_{2,p}^{q} \\\\ \n",
    "\\vdots & \\vdots  & \\cdots & \\vdots & \\vdots & \\cdots & \\vdots & \\vdots & \\vdots \\\\ \n",
    "1 & x_{N,1} & \\cdots & x_{N,1}^{q} & x_{N,2} & \\cdots & x_{N,2}^{q} & \\cdots & x_{N,p}^{q}\n",
    "\\end{pmatrix}, \\;\\;\\;\\;\\; \\mathbf{w}=\\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\ \n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_{pq} \\\\\n",
    "\\end{pmatrix}, \\;\\;\\;\\;\\; $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$는 기존 feature 데이터에서 1만으로 이루어진 column과 $p$번째 feature$q$차항까지의 $x_{i}^{q}$ 데이터의 column들을 추가한 형태를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 1> 고차항 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1개의 column 벡터를 가지는 feature 데이터를 입력하였을 때, 3차항으로 모델을 구현하고 그 예측값을 출력하여 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_feature: \n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "\n",
      "X: \n",
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  2.  4.  8.]\n",
      " [ 1.  3.  9. 27.]\n",
      " [ 1.  4. 16. 64.]]\n",
      "\n",
      "Y: \n",
      "[[3.1]\n",
      " [4.9]\n",
      " [7.2]\n",
      " [8.9]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# feature 데이터\n",
    "X_feature = np.array([1, 2, 3, 4]).reshape((-1,1))\n",
    "\n",
    "# 3차항 선형 회귀 X 선언\n",
    "X = np.c_[np.ones((X_feature.shape[0],1)),X_feature]\n",
    "for i in range(2):\n",
    "    X = np.c_[X,X_feature**(i+2)]\n",
    "\n",
    "# label 데이터\n",
    "Y = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))\n",
    "\n",
    "print(\"X_feature: \\n{}\\n\".format(X_feature))\n",
    "print(\"X: \\n{}\\n\".format(X))\n",
    "print(\"Y: \\n{}\\n\".format(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F(X): \n",
      "[[ 4.]\n",
      " [15.]\n",
      " [40.]\n",
      " [85.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 column 설정\n",
    "w = np.ones((X.shape[1],1))\n",
    "\n",
    "# 다항 선형 모델 함수\n",
    "def F_X(w,X):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "print(\"F(X): \\n{}\\n\".format(F_X(w,X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss 함수 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 loss 함수는 다중 선형 회귀와 똑같은 형태로 정리됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss 함수\n",
    "\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "Loss(\\mathbf{w})&=\\frac{1}{N}\\sum_{i=1}^{N}(y_i-f(\\mathbf{x_i}))^{2} \\\\\n",
    "&=\\frac{1}{N}(\\mathbf{y}-X\\mathbf{w})^{T}(\\mathbf{y}-X\\mathbf{w})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과적으로 least square solution 또한 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 고차항 선형 회귀 파라미터 least square solution\n",
    "\n",
    "> $$\\mathbf{\\widehat{w}}=(X^{T}X)^{-1}X^{T}\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**least square 해의 존재 문제** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$는 N x (qp+1) 행렬이기에 $(X^{T}X)$는 (qp+1) x (qp+1) 행렬입니다.\n",
    "\n",
    "그런데 만약 샘플의 수 N이 (qp+1)보다 작게 된다면 $X$의 full rank의 값이 $N$이 되기에 온전한 해를 얻을 수 없습니다.\n",
    "\n",
    "따라서 위의 `<예제 1>`에서의 4개의 샘플에서 $p=1,q=3$이기에 $qp+1=4$이기에 $q$를 늘리는 순간 least square solution은 의미가 없습니다.\n",
    "\n",
    "하지만 일반적으로 샘플 수 $N$은 매우 크기에 큰 문제가 되지는 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 2> 고차항 선형 회귀 구현 - least square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<예제 1>`에 이어서 고차항 선형 회귀 최적의 파라미터를 least square로 구하고 최소화 된 loss 값을 출력해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.545584617537555e-22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# feature 데이터\n",
    "X_feature = np.array([1, 2, 3, 4]).reshape((-1,1))\n",
    "\n",
    "# 3차항 선형 회귀 X 선언\n",
    "X = np.c_[np.ones((X_feature.shape[0],1)),X_feature]\n",
    "for i in range(2):\n",
    "    X = np.c_[X,X_feature**(i+2)]\n",
    "\n",
    "# label 데이터\n",
    "Y = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))\n",
    "\n",
    "\n",
    "w = np.dot(np.linalg.inv(np.dot(X.transpose(),X)),np.dot(X.transpose(),Y))\n",
    "\n",
    "# 다중 선형 모델 함수\n",
    "def F_X(w,X):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "\n",
    "def loss(f_x, label_data):\n",
    "    error = label_data - f_x\n",
    "    ls = np.mean(error**2)\n",
    "    return ls\n",
    "\n",
    "print(\"loss: {}\\n\".format(loss(F_X(w,X),Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 고차항 선형 회귀 class 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 수행된 것을 확인해보면 고차항 선형 회귀의 class는 $X$를 만들어주는 것 이외에는 다중 선형 회귀의 class와 다를게 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 3> 고차항 선형 회귀 class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class는 least square를 사용했던 단순 선형 회귀 구조와 비슷하게 초기화, 학습, 예측, loss 함수로 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class poly_linaer_regression:\n",
    "    # 초기화 함수\n",
    "    def __init__(self, initial_w, d):\n",
    "        self.w = initial_w\n",
    "        # degree는 고차항의 수를 의미\n",
    "        self.degree = d\n",
    "        \n",
    "    # 학습 함수    \n",
    "    def fit(self, feature, label):\n",
    "        \n",
    "        # 고차항을 추가하여 X를 만드는 과정\n",
    "        X = np.c_[np.ones((feature.shape[0],1)),feature]\n",
    "        for i in range(self.degree-1):\n",
    "            X = np.c_[X,feature**(i+2)]\n",
    "        self.w = np.dot(np.linalg.inv(np.dot(X.transpose(),X)),np.dot(X.transpose(),label))\n",
    "\n",
    "        \n",
    "    # 예측값 계산 함수\n",
    "    def predict(self, feature):\n",
    "        \n",
    "        # 고차항을 추가하여 X를 만드는 과정\n",
    "        X = np.c_[np.ones((feature.shape[0],1)),feature]\n",
    "        for i in range(self.degree-1):\n",
    "            X = np.c_[X,feature**(i+2)]\n",
    "        prediction = np.dot(X,self.w)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    # loss 값 계산 함수\n",
    "    def loss(self, feature, label):\n",
    "        \n",
    "        error = label - self.predict(feature)\n",
    "        ls = np.mean(error**2)\n",
    "        \n",
    "        return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 4> 고차항 선형 회귀 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<예제 3>` 고차항 선형 회귀 class에서 구현한 class를 사용하여 학습 과정을 수행해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.545584617537555e-22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 학습용 데이터\n",
    "feature_data = np.array([1, 2, 3, 4]).reshape((-1,1))\n",
    "label_data = np.array([3.1,4.9,7.2,8.9]).reshape((-1,1))\n",
    "\n",
    "# 파라미터 0으로 초기화\n",
    "w = np.zeros((feature_data.shape[1]+1,1))\n",
    "            \n",
    "# 다중 선형 회귀 모델 불러오기 및 초기화\n",
    "model = poly_linaer_regression(w,3)\n",
    "\n",
    "# 학습 수행\n",
    "model.fit(feature_data, label_data)\n",
    "\n",
    "print(\"loss: {}\\n\".format(model.loss(feature_data, label_data))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. scikit-learn 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "고차항 선형회귀와 동일한 방식으로 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 5> `PolynomialFeatures` 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "고차항 선형 회귀를 scikit-learn으로 수행하기 위해서 `PolynomialFeatures`을 사용하여 feature 데이터의 3차항 column 배열을 추가해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature data: \n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "\n",
      "X: \n",
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  2.  4.  8.]\n",
      " [ 1.  3.  9. 27.]\n",
      " [ 1.  4. 16. 64.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "feature_data = np.array([1, 2, 3, 4]).reshape((-1,1))\n",
    "print(\"feature data: \\n{}\\n\".format(feature_data))\n",
    "\n",
    "# 3차항 선형 회귀에 해당되는 3를 초기값으로 입력\n",
    "poly = PolynomialFeatures(3)\n",
    "\n",
    "# fit 함수를 사용하여 X로 변환 작업을 수행\n",
    "X = poly.fit_transform(feature_data)\n",
    "print(\"X: \\n{}\\n\".format(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PolynomialFeatures`을 사용하여 $X$가 준비되었다면 단순 선형 회귀와 같은 방법으로 학습을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 6>  scikit-learn을 사용한 고차항 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<예제 4>` 에서와 같은 데이터를 사용하여 이번엔 scikit-learn의 loss 값을 출력해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.17689969051289e-30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 모델 설정\n",
    "sci_model = LinearRegression()\n",
    "\n",
    "# 학습 수행\n",
    "sci_model.fit(X, label_data)\n",
    "\n",
    "# scikit-learn 에서는 loss 함수가 모델안에 내정되어 있지 않기에 정의\n",
    "def loss(prediction, label):\n",
    "        \n",
    "    error = label - prediction\n",
    "    ls = np.mean(error**2)\n",
    "\n",
    "    return ls\n",
    "\n",
    "print(\"loss: {}\\n\".format(loss(sci_model.predict(X), label_data))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
