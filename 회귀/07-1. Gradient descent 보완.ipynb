{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 단순 선형 회귀 - Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 목표\n",
    "- gradient descent의 문제점을 해결할 수 있는 방법에 대해서 학습합니다.\n",
    "- gradient descent를 사용한 단순 선형 회귀 알고리즘을 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gradient descent 보완\n",
    "1. Local minima 문제\n",
    "2. Learning rate 문제\n",
    "3. 초기 값 문제\n",
    "\n",
    "### 2. Gradient descent를 사용한 단순 선형 회귀\n",
    "1. 단순 선형 회귀 class 구현\n",
    "2. Gradient descent 보완\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient descent 보완"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Local minima 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 2장에서 살펴 보았던 gradient descent 문제점 3가지를 다루며 해결책을 생각해 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local minima 문제는 실제 최소 값(global minima) 외에 지역 최소 값(local minima)이 존재한다면 gradient descent는 실제 최소 값에 도달하지 못하고 지역 최소 값에서 멈추는 것을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/3-1-2.png\" width=\"50%\" height=\"50%\" title=\"local1\" alt=\"local1\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent 업데이트 룰을 따르게 된다면 어느 위치에 있던 다음 업데이트 방향은 내려가는 방향으로 잡히게 됩니다.\n",
    "\n",
    "위 그림에서 3번 위치에서 시작하게 된다면 큰 문제 없이 global minima 에 도착합니다.\n",
    "\n",
    "하지만 1, 2번 위치에서 움직이게 되면 local minima에 도착하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 해결 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE로 정의된 Loss 함수이기에 선형 함수의 파라라미터에 대해서 정리하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss 함수\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "Loss(w_0, w_1)&=\\frac{1}{N}\\sum_{i=1}^{N}(y_i-f(x_i))^{2} \\\\\n",
    "&=\\frac{1}{N}\\sum_{i=1}^{N}(y_i-(w_O+w_1 x_i))^{2}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기 값 $w_1$에 대한 Loss 값은 2차 함수 관계 있기에 아래의 그림과 같은 형태로 그려집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "w1 = np.arange(-12,10,0.1)\n",
    "\n",
    "loss_w1 = 1*w1**2 + 2*w1 + 1\n",
    "\n",
    "plt.plot(w1, loss_w1)\n",
    "plt.xlabel('w1')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차 함수의 특성상 local minima 지점이 생길 수 없기에 gradient descent 과정을 통하여 항상 global minima를 찾을 수 있습니다.\n",
    "\n",
    "y 절편에 대해서도 같은 방식으로 증명이 가능합니다.\n",
    "\n",
    "하지만, Loss 함수가 MSE가 아닌 다른 함수로 정의가 되면 이야기가 달라집니다.\n",
    "\n",
    "만약 Loss 함수가 아래와 같이 정의된다면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 기울기의 4차식 형태 loss 함수\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "Loss(w_0, w_1)&=\\frac{1}{N}\\sum_{i=1}^{N}(y_i-f(x_i))^{4} \\\\\n",
    "&=\\frac{1}{N}\\sum_{i=1}^{N}(y_i-(w_O+w_1 x_i))^{4}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기에 대해서 4차 함수가 됩니다. 4차 함수는 2개의 local minima와 1개의 global minima를 가질 수 있기에 문제가 생길 수 있습니다.\n",
    "\n",
    "따라서 선형 회귀 문제에서 Loss 함수를 MSE로 잡는 것은 오차를 양수로 표현하는 것 이외로 local minima 문제를 해결한다는 장점이 있습니다.\n",
    "\n",
    "만일 비선형 회귀 문제나 Loss 함수가 비선형인 문제를 다룰 경우에는 local minima를 해결하기 위하여 stochastic gradient descent (SGD) 방식을 쓰거나 다양한 최적화 기법을 통하여 해결합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Learning rate 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate 설정은 경사 방향으로 얼마나 많이 움직일 것인가를 의미합니다.\n",
    "\n",
    "어떤 문제가 생기는지 알아보기 위해서 먼저 Learning rate가 너무 높이 설정된 경우를 살펴 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/3-1-3.png\" width=\"50%\" height=\"50%\" title=\"lr1\" alt=\"lr1\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate가 너무 높이 설정 되었다면 위 그림과 같이 Global minima 지점을 지나 다른 곳으로 이동합니다.\n",
    "\n",
    "움직이는 지점에서의 방향은 gradient 값으로 아래로 미끄러졌지만 이동 거리가 너무 길기에 내려갔다가 다시 올라가는 현상이 벌어지게 됩니다.\n",
    "\n",
    "2번째 이동했던 지점에서 만일 learning rate를 줄일 수 있었다면 Global minima 에 도착할 수도 있었을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번엔 Learning rate가 너무 낮게 설정된 경우를 살펴 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/3-1-4.png\" width=\"50%\" height=\"50%\" title=\"lr2\" alt=\"lr2\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate가 너무 낮게 설정되었다면 위 그림과 같이 매우 느린 움직임으로 Global minima에 다가가고 있음을 볼 수 있습니다.\n",
    "\n",
    "Learning rate가 너무 높아서 Global minima를 지나칠 일은 없지만 너무 작은 값을 값게 되면 매우 많은 이동이 필요하게 됩니다.\n",
    "\n",
    "이는 계산양 증가라는 문제를 부를 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 해결 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate가 너무 낮을 때에는 높이고 너무 높은 경우에는 낮게하는 유동적 조절이 필요합니다.\n",
    "\n",
    "이를 해결하는 방식으로는 momentum, adam 과 같은 방식이 있지만 이 내용들은 신경망 학습 부분에서 제대로 다루겠습니다.\n",
    "\n",
    "위 방식 대신에 간단한 해결 방법을 알아 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. iteration 횟수에 따라 learning rate 줄이기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음부터 큰 learning rate를 잡고 업데이트 iteration 횟수에 따라 learning rate를 감소시키는 방법입니다.\n",
    "\n",
    "너무 큰 learning rate의 경우 반복 횟수에 따라 줄어들기에 global minima에 도달할 수 있습니다.\n",
    "\n",
    "learning rate를 줄이는 방법으로는 아래의 수식처럼 일정 비율로 감소 시키는 방법이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### learning rate 감소법\n",
    "\n",
    "> $$[w_{0}^{t+1}, w_{1}^{t+1}] = [w_{0}^{t}, w_{1}^{t}] - \\frac{\\alpha}{iteration} \\triangledown Loss(w_{0}^{t}, w_{1}^{t})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$iteration$은 업데이트 횟수를 의미합니다.\n",
    "\n",
    "기존 고정된 learning rate 보다는 성능이 좋을 수 있지만 이 또한 문제점이 많습니다.\n",
    "\n",
    "시작 점 위치에 따라 global minima 위치를 벗어나 점점 느리게 수렴할 수 있어 오히려 반복 횟수가 늘어날 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Loss 값 증가시 learning rate 줄이기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. iteration 횟수에 따라 learning rate 줄이기` 에서 소개한 방법은 learning rate가 iteration 횟수에만 영향을 받아 줄어드는 단점이 있었습니다.\n",
    "\n",
    "이를 보완하기 위해서 Loss 값이 증가하는 경우에만 learning rate를 줄여 빠른 global minima를 향하여 빠른 속도로 이동할 수 있도록 합니다.\n",
    "\n",
    "이 방법은 아래와 같은 알고리즘으로 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 파라미터 초기화\n",
    "2. 반복문 시작 (업데이트 된 loss 값의 차이가 매우 적을 때 까지 반복)\n",
    "\n",
    "    1) gradient 계산\n",
    "    \n",
    "    2) 파라미터 업데이트 계산\n",
    "    \n",
    "    3) 전 파라미터로 계산 된 loss 값과 현재 파라미터의 loss 값 계산 후 비교하여 현재 파라미터의 loss 값이 더 크다면 learning rate를 줄임\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 1> learning rate 문제 해결 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2-2. <예제 2> gradient descent 알고리즘 구현`에서 구현한 gradient descent 알고리즘에서 learning rate 문제를 해결해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diverges\n",
      "updates learning rate * 50%\n",
      "diverges\n",
      "updates learning rate * 50%\n",
      "diverges\n",
      "updates learning rate * 50%\n",
      "diverges\n",
      "updates learning rate * 50%\n",
      "saturates\n",
      "number of iteration : 92\n",
      "loss : 0.016010926554666734\n",
      "W0 : 1.0612134243018572\n",
      "W1 : 1.9831921656317235\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1차 선형 모델 함수 정의\n",
    "def linear_model(w_0, w_1, feature_data):\n",
    "    f_x = w_0 + w_1*feature_data\n",
    "    return f_x\n",
    "\n",
    "# loss 함수 정의\n",
    "def loss(f_x, label_data):\n",
    "    error = label_data - f_x\n",
    "    ls = np.mean(error**2)\n",
    "    return ls\n",
    "\n",
    "# gradient 함수 정의\n",
    "def gradient(w0, w1, feature_X, label_Y):\n",
    "    \n",
    "    gradient_w0 = -2/(feature_X.size) * np.sum(label_Y - (w0+w1*feature_X))\n",
    "    gradient_w1 = -2/(feature_X.size) * np.sum((label_Y - (w0+w1*feature_X))*(feature_X))\n",
    "    \n",
    "    return np.array([gradient_w0, gradient_w1])\n",
    "\n",
    "# 학습용 데이터 설정\n",
    "feature_data = np.array([1,2,3,4]).reshape((-1,1))\n",
    "label_data = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))\n",
    "\n",
    "\n",
    "# 파라미터 초기화\n",
    "w0 = 0\n",
    "w1 = 0\n",
    "\n",
    "# 현재 파라미터로 계산한 loss 값 저장 \n",
    "loss1 = loss(linear_model(w0, w1, feature_data), label_data)\n",
    "\n",
    "# 반복 횟수를 세기 위하여 변수 초기화\n",
    "num_iter = 0\n",
    "\n",
    "# learning rate 초기화\n",
    "lr = 1\n",
    "\n",
    "while True:\n",
    "    \n",
    "    num_iter = num_iter + 1\n",
    "    \n",
    "    gd = gradient(w0, w1, feature_data, label_data)\n",
    "        \n",
    "    w0 = w0 - lr*gd[0]\n",
    "    w1 = w1 - lr*gd[1]\n",
    "    \n",
    "    if loss1 < loss(linear_model(w0, w1, feature_data), label_data):\n",
    "        print(\"diverges\")\n",
    "        print(\"updates learning rate * 50%\")\n",
    "        status = 'diverges'\n",
    "        lr = lr/2\n",
    "\n",
    "    if abs(loss1 - loss(linear_model(w0, w1, feature_data), label_data)) < 0.00001:\n",
    "        print(\"saturates\")\n",
    "        print(\"number of iteration : {}\".format(num_iter))\n",
    "        print(\"loss : {}\".format(loss1))\n",
    "        print(\"W0 : {}\".format(w0))\n",
    "        print(\"W1 : {}\".format(w1))\n",
    "        status = 'converges'\n",
    "        break\n",
    "        \n",
    "    loss1 = loss(linear_model(w0, w1, feature_data), label_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. 초기 값 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 회귀 파라미터의 초기값 설정은 목적지에서 얼마나 가까운 곳에서 시작을 하는 것을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/3-1-5.png\" width=\"50%\" height=\"50%\" title=\"ini1\" alt=\"ini1\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 2가지 시작 점(initial point)에 따른 gradient descent 과정을 보여주고 있습니다.\n",
    "\n",
    "initial point 1에 비해서 inital point 2는 global minima와 가깝기에 보다 빠르게 도달할 수 있음을 알 수 있습니다.\n",
    "\n",
    "당연하게도 시작 점을 global minima와 가깝게 잡을수록 보다 빠르게 gradient descent 과정을 마칠 수 있습니다.\n",
    "\n",
    "하지만 global minima 가 어딨는지를 모르기 때문에 시작 점 또한 어디서부터 시작해야 하는지 기준이 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 해결 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시작 점을 잡는 방식으로는 가우시안 샘플링과 같은 방식이 있지만 이 내용들 또한 신경망 학습 부분에서 제대로 다루겠습니다.\n",
    "\n",
    "위 방식 대신에 간단한 해결 방법을 알아 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사전 정보 이용**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 시작 점 위치를 잡을 때는 랜덤하게 값을 뽑아서 gradient descent 방식을 수행하게 됩니다.\n",
    "\n",
    "하지만 탐색적 데이터 분석을 통하여 특성 데이터와 레이블 데이터 간의 상관 관계를 출력했다면 global minima 위치에 대한 정보를 어느정도 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/3-1-6.png\" width=\"40%\" height=\"40%\" title=\"ini2\" alt=\"ini2\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림과 같이 특성 데이터와 레이블 데이터 간의 분포를 알 수 있다면 간단하게 기울기가 양수 값을 가질 것이라는 정보를 얻을 수 있습니다.\n",
    "\n",
    "이렇게 데이터가 가지고 있는 상관 관계를 통한 사전 정보를 통하여 시작 점의 위치를 대략적으로 유추할 수 있고 이것은 시작 점을 잡는데 큰 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 2> 초기 값 문제 해결 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<예제 1> learning rate 문제 해결 알고리즘`에서 수행한 코드를 바탕으로 `w0, w1`을 초기화를 어떻게 설정하냐에 따라 변하는 iteration 수를 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diverges\n",
      "updates learning rate * 50%\n",
      "diverges\n",
      "updates learning rate * 50%\n",
      "diverges\n",
      "updates learning rate * 50%\n",
      "diverges\n",
      "updates learning rate * 50%\n",
      "saturates\n",
      "number of iteration : 58\n",
      "loss : 0.016019553663658693\n",
      "W0 : 1.1394225687172648\n",
      "W1 : 1.9565915191845342\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1차 선형 모델 함수 정의\n",
    "def linear_model(w_0, w_1, feature_data):\n",
    "    f_x = w_0 + w_1*feature_data\n",
    "    return f_x\n",
    "\n",
    "# loss 함수 정의\n",
    "def loss(f_x, label_data):\n",
    "    error = label_data - f_x\n",
    "    ls = np.mean(error**2)\n",
    "    return ls\n",
    "\n",
    "# gradient 함수 정의\n",
    "def gradient(w0, w1, feature_X, label_Y):\n",
    "    \n",
    "    gradient_w0 = -2/(feature_X.size) * np.sum(label_Y - (w0+w1*feature_X))\n",
    "    gradient_w1 = -2/(feature_X.size) * np.sum((label_Y - (w0+w1*feature_X))*(feature_X))\n",
    "    \n",
    "    return np.array([gradient_w0, gradient_w1])\n",
    "\n",
    "# 학습용 데이터 설정\n",
    "feature_data = np.array([1,2,3,4]).reshape((-1,1))\n",
    "label_data = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))\n",
    "\n",
    "\n",
    "# 파라미터 초기화\n",
    "\"\"\"\n",
    "feature 정보를 보고 대충의 파라미터 값을 예측하여 초기화\n",
    "기울기는 양수이지만 큰 폭은 아니기에 1로 가정\n",
    "y절편 또한 1로 가정\n",
    "\"\"\"\n",
    "w0 = 1\n",
    "w1 = 1\n",
    "\n",
    "# 현재 파라미터로 계산한 loss 값 저장 \n",
    "loss1 = loss(linear_model(w0, w1, feature_data), label_data)\n",
    "\n",
    "# 반복 횟수를 세기 위하여 변수 초기화\n",
    "num_iter = 0\n",
    "\n",
    "# learning rate 초기화\n",
    "lr = 1\n",
    "\n",
    "while True:\n",
    "    \n",
    "    num_iter = num_iter + 1\n",
    "    \n",
    "    gd = gradient(w0, w1, feature_data, label_data)\n",
    "        \n",
    "    w0 = w0 - lr*gd[0]\n",
    "    w1 = w1 - lr*gd[1]\n",
    "    \n",
    "    if loss1 < loss(linear_model(w0, w1, feature_data), label_data):\n",
    "        print(\"diverges\")\n",
    "        print(\"updates learning rate * 50%\")\n",
    "        status = 'diverges'\n",
    "        lr = lr/2\n",
    "\n",
    "    if abs(loss1 - loss(linear_model(w0, w1, feature_data), label_data)) < 0.00001:\n",
    "        print(\"saturates\")\n",
    "        print(\"number of iteration : {}\".format(num_iter))\n",
    "        print(\"loss : {}\".format(loss1))\n",
    "        print(\"W0 : {}\".format(w0))\n",
    "        print(\"W1 : {}\".format(w1))\n",
    "        status = 'converges'\n",
    "        break\n",
    "        \n",
    "    loss1 = loss(linear_model(w0, w1, feature_data), label_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
