회귀, 분류 담당 강사님 : 조해선 ? 고해선 ? ?? 

( 성균관대 컴공 ? 석사, 자연어, 네이버 연구인턴, sds, lg 등등)

-------

# 1. Regression

[ 면접 질문 ]

1 ) 다양한 regression 기법

2 ) gradient descent / normal equation 비교

3 ) linear regression 시 regularization 방법에 대해 ( 릿지, 라쏘 ) 각각을 언제 쓰면 좋을지에 대한 설명

4 ) 전통적 regression과 딥러닝의 MLP의 차이점이 무엇인가? 

- 전통적 Regression : regression의 경우는 인간이 개입하는 것이 많다. 사람이 일차함수, 이차함수, 지수함수 등을 결정한 이후에 parameter를 조정
- 딥러닝 : 어떤 함수를 선택할지 조차도 알아서 하도록
- * 더 알아 볼 것 * * 

5 ) 언제 MSE, MAE를 사용해야 하는가 ?

* MAE : outlier에 penalty 부여
* 

---------

## 1 ) regression  정의

인풋, 아웃풋에 대해 가장 근사한 선을 찾는 것

y = w0 + w1 * x

변수가 늘어날 수록 feature가 늘어난다. 

x, x**2 등 .. 

regression 모델을 학습할 때 train, test로 나누어 학습하는데, 이 때 train data와 test data가 무엇인가 ?

1 ) train data : 학습용

```
얻고자 하는 것 : 다양한 값들 중에서  w0, w1의 값을 얻기 위한 것

supervised learning 또한 위의 값들을 얻기 위함

= parameter learning
```

2 ) test data : 테스트용

```
얻고자 하는 것 : y hat을 예측하는 것

내가 학습한 파라미터가 좋은지 알아가는 단계
```

-----

## 2 ) Linear  vs  Logistic  vs  Softmax Regression 비교

linear regression : continuos 한 아웃풋

logistic, softmax : distribution한 아웃풋

logistic :0 , 1로만 구성 ( pass, fail )

softmax : output의 label이 여러개 ( a+ , a, b+ , ..)

---

## 3 ) 용어정리

feature : input의 값

label : output의 값

![image-20200709142327200](0709%20%EC%88%98%EC%97%85.assets/image-20200709142327200.png)

X = N * p 의 데이터

p : feature 개수 (column)

N : 데이터의 개수 (row)

-------

# 2. 회귀 분석 모델

## 1 ) 선형 회귀

단변수 regression, 다변수 regression

+ normal equation 또한 행렬식으로 푸는 방법인데 이 방법이 기초가된다

y = w0 + w1 * x ( 현재 스칼라 값 )

![image-20200709151039343](0709%20%EC%88%98%EC%97%85.assets/image-20200709151039343.png)

y = w0 * 1 + w1 * x1 + w2 * x2 + ... + wp * xp

![image-20200709151245743](0709%20%EC%88%98%EC%97%85.assets/image-20200709151245743.png)



## 2 ) 오차와 loss함수

error : 실제값 - 예측값

* MSE 

![image-20200709152920976](0709%20%EC%88%98%EC%97%85.assets/image-20200709152920976.png)

```python
import numpy as np


feature_data = np.array([1,2,3,4]).reshape((-1,1))
label_data = np.array([3.1,4.9,7.2,8.9]).reshape((-1,1))

f_x = 2*feature_data + 1

def loss(f_x, label_data):
    error = label_data - f_x
    ls = np.mean(error**2)
    return ls

print("loss: {}\n".format(loss(f_x,label_data)))
```



* MAE  

* ![image-20200709152931881](0709%20%EC%88%98%EC%97%85.assets/image-20200709152931881.png)

  ```python
  import numpy as np
  
  
  feature_data = np.array([1,2,3,4]).reshape((-1,1))
  label_data = np.array([3.1,4.9,7.2,8.9]).reshape((-1,1))
  
  f_x = 2*feature_data + 1
  
  def loss(f_x, label_data):
      error = label_data - f_x
      ls = np.mean(np.abs(error))
      return ls
  
  print("loss: {}\n".format(loss(f_x,label_data)))
  ```

  

  언제 MSE, MAE를 사용해야 하는가 ?

# 3. 단순 선형 회귀

## 1 ) least square

: gradient descent와 다르게 한번에 답을 찾아내고자 한다. ( <-> 점진적 발전 ) 컴퓨터는 행렬 계산에 많은 시간이 들기 때문에 gradient descent 방법이 나오게 되었다. 딥러닝에서는 약간의 오차를 허용하더라도 빠른 것이 중요하기 때문에 gradient descent  선호함.

loss가 최소가되는 w0, w1이 무엇인지 찾아보자

![image-20200709155623592](0709%20%EC%88%98%EC%97%85.assets/image-20200709155623592.png)

미분했을 때 값이 0일 때의 w0와 w1을 찾는다.

![image-20200709160251670](0709%20%EC%88%98%EC%97%85.assets/image-20200709160251670.png)

![image-20200709160503154](0709%20%EC%88%98%EC%97%85.assets/image-20200709160503154.png)

![image-20200709160517306](0709%20%EC%88%98%EC%97%85.assets/image-20200709160517306.png)

![image-20200709161001322](0709%20%EC%88%98%EC%97%85.assets/image-20200709161001322.png)

앞에 역행렬을 곱해주자

: 역행렬이 존재하는지 안하는지 판단해야한다.

역행렬이 0일때는 존재하지 않다.

매번 얘가 존재하는지 안하는지 판단하기 위해서는 ? 

모든 xi는 같은 값을 가져야만 역행렬이 존재하지 않기 때문에 대부분의 행렬은 역행렬을 가진다고 말할 수 있다.

![image-20200709161049901](0709%20%EC%88%98%EC%97%85.assets/image-20200709161049901.png)

## 2 ) 단순 선형 회귀 class

* 직접해보기

```python
import numpy as np

class simple_linaer_regression:
    # 초기화 함수
    def __init__(self, initial_w0, initial_w1):
        self.w1 = initial_w1
        self.w0 = initial_w0
        
    # 학습 함수    
    def fit(self, feature, label):
        
        leastsquare1 = np.array([[feature.size, np.sum(feature)],[np.sum(feature), np.sum(feature**2)]])
        leastsquare2 = np.array([[np.sum(label)],[np.sum(feature*label)]])
        
        W = np.dot(np.linalg.inv(leastsquare1),leastsquare2)
        self.w0 = W[0]
        self.w1 = W[1]
        
    # 예측값 계산 함수
    def predict(self, feature):
        
        prediction = self.w1*feature + self.w0
        
        return prediction
    
    # loss 값 계산 함수
    def loss(self, feature, label):
        
        error = label - self.predict(feature)
        ls = np.mean(error**2)
        
        return ls
```

* scikit-learn 모듈

```python
import numpy as np
from sklearn.linear_model import LinearRegression

feature_data = np.array([1,2,3,4]).reshape((-1,1))
label_data = np.array([3.1,4.9,7.2,8.9]).reshape((-1,1))

# 모델 설정
sci_model = LinearRegression()

# 학습 수행
sci_model.fit(feature_data, label_data)

# scikit-learn 에서는 loss 함수가 모델안에 내정되어 있지 않기에 정의
def loss(prediction, label):
        
    error = label - prediction
    ls = np.mean(error**2)

    return ls

print("loss: {}\n".format(loss(sci_model.predict(feature_data), label_data))) 
```

## 3 ) 단순 선형 회귀 평가

루트를 씌어주는 이유 : 수치를 작게하려고

* RMSE

![image-20200709163653667](0709%20%EC%88%98%EC%97%85.assets/image-20200709163653667.png)

* RMAE

![image-20200709163709686](0709%20%EC%88%98%EC%97%85.assets/image-20200709163709686.png)

* R2 : 식의 의미를 알고 있어야한다.

* 의미 : 1이면 설명하기 좋고 0이면 설명력이 떨어진다

  y평균과 y hat의 값이 같다 -> 더한 값도 1이다 -> 의미가 없다 

  y 평균과 y hat의 값이 다르다 -> 더한 값이 0에 가까우면 -> 의미가 있따 -> R2값이1ㅇㅔ 가깝다

  

![image-20200709163840781](0709%20%EC%88%98%EC%97%85.assets/image-20200709163840781.png)

: R2 의 단점은 feature가 많을 수록 overfitting이 발생한다. 따라서 이를 보완하기 위해 adjusted R2가 나오게 되었따.

* adjusted R2

![image-20200709163931919](0709%20%EC%88%98%EC%97%85.assets/image-20200709163931919.png)

n < p 인 경우에  분모가 음수로 변하기 때문에 페널티를 부여

n >>> p 인 경우에 underfitting이 일어나기 때문에 수치를 낮춰준다.

