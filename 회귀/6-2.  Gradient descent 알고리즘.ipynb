{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gradient descent 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 목표\n",
    "- Gradient descent 알고리즘을 이해하고 구현합니다.\n",
    "- Gradient descent 장단점을 파악합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loss 함수 경사 하강하기\n",
    "1. loss 함수 그래프와 최소 값 지점\n",
    "2. 경사 하강하기\n",
    "\n",
    "### 2. Gradient descent 알고리즘\n",
    "1. Gradient 값 계산\n",
    "2. 업데이트 방식\n",
    "3. 알고리즘 예시\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient descent 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Gradient descent 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알고리즘을 설명하기 앞서 가장 가파른 내려가는 방향을 의미하는 gradient 값을 계산하는 방식을 알아봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient 값 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기 벡터를 의미하는 gradient는 선형 함수의 각 파라미터들의 편미분으로 구성된 열벡터로 정의합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 선형 함수 파라미터의 gradient   \n",
    "\n",
    "> $$gradient= \\triangledown Loss = \\begin{pmatrix}\n",
    "\\frac{\\partial Loss}{\\partial w_0} \\\\ \n",
    "\\frac{\\partial Loss}{\\partial w_1} \\\\ \n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial Loss}{\\partial w_p}\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 파라미터 별 편미분 값으로 정의 되었기에 변화폭이 큰 파라미터 값의 방향으로 변합니다.\n",
    "\n",
    "1차 함수에서는 선형 함수의 파라미터가 기울기와 y절편 뿐이기에 다음과 같이 계산됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1차 함수 파라미터의 gradient\n",
    "\n",
    "> $$gradient= [\\frac{\\partial Loss}{\\partial w_0},\\frac{\\partial Loss}{\\partial w_1}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 loss 함수를 편미분해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss 함수\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "Loss(w_0, w_1)&=\\frac{1}{N}\\sum_{i=1}^{N}(y_i-f(x_i))^{2} \\\\\n",
    "&=\\frac{1}{N}\\sum_{i=1}^{N}(y_i-(w_O+w_1 x_i))^{2}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 $w_0$에 대해서 편미분을 하게 되면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss함수 편미분\n",
    "\n",
    "> $$\\frac{\\partial Loss}{\\partial w_0} = \\frac{2}{N}\\sum_{i=1}^{N}(y_i-(w_O+w_1 x_i))(-1)$$\n",
    "\n",
    "> $$\\frac{\\partial Loss}{\\partial w_1} = \\frac{2}{N}\\sum_{i=1}^{N}(y_i-(w_O+w_1 x_i))(-x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 1> gradient 계산 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 미분된 수식을 사용하여 gradient 계산 함수를 구현해봅시다.\n",
    "\n",
    "데이터와 모델은 `1-1. <예제 1>`에서 사용한 것과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.05, -15.05])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient(w0, w1, feature_X, label_Y):\n",
    "    \n",
    "    gradient_w0 = -2/(feature_X.size) * np.sum(label_Y - (w0+w1*feature_X))\n",
    "    gradient_w1 = -2/(feature_X.size) * np.sum((label_Y - (w0+w1*feature_X))*(feature_X))\n",
    "    \n",
    "    return np.array([gradient_w0, gradient_w1])\n",
    "\n",
    "\n",
    "feature_data = np.array([1,2,3,4]).reshape((-1,1))\n",
    "label_data = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))\n",
    "\n",
    "gradient(1,1,feature_data,label_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 업데이트 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient 계산법을 이해했으면 시작 점으로 부터 어떻게 이동하는지에 대해서 알아봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2-2-1.png\" width=\"30%\" height=\"30%\" title=\"gradient6\" alt=\"gradient6\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림과 같이 시작점에서의 좌표가 $(w_{0}^{0}, w_{1}^{0})$라 한다면 gradient 방향 및 크기에 학습율을 곱한 만큼 이동합니다.\n",
    "\n",
    "$w_0$는 y절편, $w_1$는 기울기에 해당됩니다.\n",
    "\n",
    "학습율은 갑자기 왜 등장했을까요?\n",
    "\n",
    "gradient는 방향을 알려주지만 어느정도의 크기도 가지고 있습니다.\n",
    "\n",
    "gradient의 크기는 가끔씩 너무 크거나 작아서 이동 거리가 너무 적거나 너무 커지는 경우가 발생하기에 이를 조절하고자 크기를 조절할 수 있는 학습율을 곱합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2-2-2.png\" width=\"50%\" height=\"50%\" title=\"gradient7\" alt=\"gradient7\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 시작점에서 이동한 좌표를 계산해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 시작점에서 이동한 좌표 계산\n",
    "\n",
    "> $$[w_{0}^{1}, w_{1}^{1}] = [w_{0}^{0}, w_{1}^{0}] - \\alpha \\triangledown Loss(w_{0}^{0}, w_{1}^{0})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 업데이트가 진행 되었으면, 정해진 반복 횟수까지 계속 업데이트를 합니다.\n",
    "\n",
    "이 과정을 일반화 하면 아래와 같이 표현 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gradient descent를 사용한 파라미터 업데이트 일반화\n",
    "\n",
    "> $$[w_{0}^{t+1}, w_{1}^{t+1}] = [w_{0}^{t}, w_{1}^{t}] - \\alpha \\triangledown Loss(w_{0}^{t}, w_{1}^{t})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 알고리즘 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent 알고리즘이 실제로 계산되어 지는 과정을 아래의 예시를 통하여 이해해 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 시작 점의 값을 $w_{0}^{0} = 0 \\;$,$w_{1}^{0} = 1$으로 설정합니다. (시작 점 값은 예시 값이고 다양한 값으로 설정 가능함) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2-2-3.png\" width=\"30%\" height=\"30%\" title=\"gradient8\" alt=\"gradient8\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 지금 위치에서의 gradient 값을 계산 합니다. \n",
    "\n",
    "> $$\\triangledown Loss(0,1)=(-1,-2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2-2-4.png\" width=\"30%\" height=\"30%\" title=\"gradient9\" alt=\"gradient9\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. y절편, 기울기 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $$w_{0}^{1} = w_{0}^{0} - \\alpha * (-1), \\;\\;$ $w_{1}^{1} = w_{1}^{0} - \\alpha * (-2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2-2-5.png\" width=\"30%\" height=\"30%\" title=\"gradient10\" alt=\"gradient10\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 최소 값 위치를 찾을 때까지 2~3 과정 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2-2-6.png\" width=\"30%\" height=\"30%\" title=\"gradient11\" alt=\"gradient11\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 2> gradient descent 알고리즘 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<예제 1>`에 이어서 gradient descent 알고리즘을 완성해 봅시다.\n",
    "\n",
    "`<예제 1>`에서는 gradient 함수를 구현하였기에 `gradient descent를 사용한 파라미터 업데이트 일반화` 수식을 사용하여 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 40.60190523375\n",
      "w0, w1 : 0.012050000000000002, 0.012050000000000002\n",
      "loss : 10.298846132274468\n",
      "w0, w1 : 0.8746631693021061, 0.8746631693021061\n",
      "loss : 2.728899480981048\n",
      "w0, w1 : 1.301971080722362, 1.301971080722362\n",
      "loss : 0.8049612734890952\n",
      "w0, w1 : 1.5136442358053832, 1.5136442358053832\n",
      "loss : 0.29997175185354485\n",
      "w0, w1 : 1.6184995951032697, 1.6184995951032697\n",
      "loss : 0.15976578769928318\n",
      "w0, w1 : 1.6704412141015637, 1.6704412141015637\n",
      "loss : 0.1172924055663782\n",
      "w0, w1 : 1.6961712464263703, 1.6961712464263703\n",
      "loss : 0.10287303346677641\n",
      "w0, w1 : 1.7089169901325372, 1.7089169901325372\n",
      "loss : 0.09735474996755633\n",
      "w0, w1 : 1.7152307786491063, 1.7152307786491063\n",
      "loss : 0.09501983287502704\n",
      "w0, w1 : 1.7183584051219858, 1.7183584051219858\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 초기화\n",
    "w0 = 0\n",
    "w1 = 0\n",
    "\n",
    "# learning rate 설정\n",
    "lr = 0.001\n",
    "\n",
    "# 1차 선형 모델 함수 정의\n",
    "def linear_model(w_0, w_1, feature_data):\n",
    "    f_x = w_0 + w_1*feature_data\n",
    "    return f_x\n",
    "\n",
    "# loss 함수 정의\n",
    "def loss(f_x, label_data):\n",
    "    error = label_data - f_x\n",
    "    ls = np.mean(error**2)\n",
    "    return ls\n",
    "\n",
    "# 반복 횟수 1000으로 설정\n",
    "for i in range(1000):\n",
    "    gd = gradient(w0,w1,feature_data,label_data)\n",
    "    w0 = w0 - lr*gd[0]\n",
    "    w1 = w1 - lr*gd[0]\n",
    "    \n",
    "    if (i%100 == 0):\n",
    "        print(\"loss : {}\".format(loss(linear_model(w0,w1,feature_data),label_data)))\n",
    "        print(\"w0, w1 : {}, {}\".format(w0,w1))\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
