![image-20200730140701452](0730%20%EC%88%98%EC%97%85.assets/image-20200730140701452.png)

![image-20200730140730065](0730%20%EC%88%98%EC%97%85.assets/image-20200730140730065.png)

![image-20200730140746987](0730%20%EC%88%98%EC%97%85.assets/image-20200730140746987.png)

![image-20200730140834203](0730%20%EC%88%98%EC%97%85.assets/image-20200730140834203.png)

![image-20200730140859386](0730%20%EC%88%98%EC%97%85.assets/image-20200730140859386.png)



# 1.강화 학습이란 ?

무엇을 할지를 배우는 것

보상을 극대화하도록 연속적인 상황에서 최선의 행동을 학습함( Non-myopic,  ex : 일어나자마자 유튜브 안보고 씻고 나갈 준비하기 )

## 1 ) 최선의 행동 학습

* 누가 : 에이전트(agent) - 행동을 하는 주체
* 어디서 : 환경(Environment) - 연속적인 상황들
* 언제 : 상태(state) - ~했을 때
* 무엇을 / 어떻게 : 행동(Action) - 이유, 보상 ( 환경이 보상을 준다 -> 다음 상태로 도달)
* 왜 : 보상(reward)

![image-20200730142131182](0730%20%EC%88%98%EC%97%85.assets/image-20200730142131182.png)

![image-20200730142150975](0730%20%EC%88%98%EC%97%85.assets/image-20200730142150975.png)

![image-20200730142156202](0730%20%EC%88%98%EC%97%85.assets/image-20200730142156202.png)

여기서 우리가 바꿀 수 있는 것은 ? ACTION !

Action이 변경됨에 따라 보상이 달라짐(보상을 극대화하는 방향으로 action을 하기 위해 learning을 진행한다)

![image-20200730142257129](0730%20%EC%88%98%EC%97%85.assets/image-20200730142257129.png)

### A. policy

* policy : 어떤 상태에서 action을 하는 함수

* ![image-20200730142506664](0730%20%EC%88%98%EC%97%85.assets/image-20200730142506664.png)

  deterministic state -> action

  stochastic state -> several actions

### B. MDP(Markov Decision Process)

* 순차적으로 결정하는 상황에 관한 모델

* Finite MDP : 상태, 행동, 보상이 유한할 경우 ( 대부분 우리가 할 수 있는 것들은 유한함 )

  ![image-20200730143001432](0730%20%EC%88%98%EC%97%85.assets/image-20200730143001432.png)

![image-20200730143020342](0730%20%EC%88%98%EC%97%85.assets/image-20200730143020342.png)

MDP에서는 내가 모든 정보들을 알고 있다.

일상생활에서 MDP로 설명할 수 없는 것들이 많다

=> POMDP : MDP와 다르게 Partially options만 주어짐, partially options가 지속적으로 무한히 주어진다 (정보가 계속 들어오니까 학습이 어떻게 될지 ?! 공부를 해보아야함...! )

### C. G

![image-20200730143337825](0730%20%EC%88%98%EC%97%85.assets/image-20200730143337825.png)

* return은 앞으로 진행되는 에피소드를 더한 결과 (위처럼 하면 미래의 불확실성이 적용되지 않음)

![image-20200730143412651](0730%20%EC%88%98%EC%97%85.assets/image-20200730143412651.png)

t시점에서 어떤 걸 하면 reward는 t+1

### D. V ( value function )

* 에이전트가 어떤 상태에서 얼마나 좋은 가치를 지니는지 측정하는 함수

* Expected Return이 높으면 좋은 가치를 지니는 것으로 표현

  ex ) 어떤 state에 도달했을 때 앞으로 일들이 일어나는데 이를 더하는 과정을 거친 후 (G), 이 값을 평균내서 뱉어내는 것 (Value function)

  ![image-20200730144058416](0730%20%EC%88%98%EC%97%85.assets/image-20200730144058416.png)

### E. Q

내가 value의 상태에서 이 행동을 취한다면 ?

![image-20200730144143449](0730%20%EC%88%98%EC%97%85.assets/image-20200730144143449.png)

![image-20200730144840786](0730%20%EC%88%98%EC%97%85.assets/image-20200730144840786.png)

![image-20200730144906785](0730%20%EC%88%98%EC%97%85.assets/image-20200730144906785.png)

### F. Bellman Expectation Equation

![image-20200730145305514](0730%20%EC%88%98%EC%97%85.assets/image-20200730145305514.png)



## 2 ) 어떻게 가치를 평가할 것인가 ?

![image-20200730152745908](0730%20%EC%88%98%EC%97%85.assets/image-20200730152745908.png)

이를 만족한 경우 Dynamic Programming 수행

* 정책 평가 : 주어진 정책에서 가치를 평가하는 과정
* 정책 향상 : 주어진 가치 함수에서 향상된 정책을 구하는 과정

이 두가지를 반복적으로 수행하여 최적의 가치 함수, 정책을 도출한다

![image-20200730152935252](0730%20%EC%88%98%EC%97%85.assets/image-20200730152935252.png)

![image-20200730152945763](0730%20%EC%88%98%EC%97%85.assets/image-20200730152945763.png)

![image-20200730153009308](0730%20%EC%88%98%EC%97%85.assets/image-20200730153009308.png)

더 이상 변하지 않는 부분이 오면 아래와 같이 바뀐뒤,

![image-20200730153736834](0730%20%EC%88%98%EC%97%85.assets/image-20200730153736834.png)

policy가 update된다. ( 정책 향상 , policy improvement  = control )

이후, 이를 이용해 또다시 value_table을 생성하고 위의 과정을 반복해본다.

그러다보면, 우리가 찾고자 했던 최적의 루트를 갖고있는 policy를 알게된다. => 최고의 보상을 갖는다.

계산하는 과정에서 만약 optimal policy가 존재한다면 ?

![image-20200730154610355](0730%20%EC%88%98%EC%97%85.assets/image-20200730154610355.png)

![image-20200730154620117](0730%20%EC%88%98%EC%97%85.assets/image-20200730154620117.png)

lookup_table의 값들이 다른 lookup_table보다 무조건 크다. 

![image-20200730155014174](0730%20%EC%88%98%EC%97%85.assets/image-20200730155014174.png)

![image-20200730155004541](0730%20%EC%88%98%EC%97%85.assets/image-20200730155004541.png)

* value iteration도 있다

  ![image-20200730155957396](0730%20%EC%88%98%EC%97%85.assets/image-20200730155957396.png)

둘을 비교해보면 

![image-20200730160017304](0730%20%EC%88%98%EC%97%85.assets/image-20200730160017304.png)

dynamic을 안다면 위와같이 policy를 구하여 평가할 수 있다

# 3. 어떻게 가치를 근사하여 평가할 것인가 ?

그렇다면 dynamic을 모른다면 어떻게 해야할까 ?

## 1 ) Monte Carlo Methods

평균값을 구하는데 무수히 많이 던지면 결국 구하게된 평균값은 policy 과정과 같아질 것이다.

![image-20200730161849496](0730%20%EC%88%98%EC%97%85.assets/image-20200730161849496.png)

value를 채워가자 (방법은 위와 동일)

![image-20200730162034800](0730%20%EC%88%98%EC%97%85.assets/image-20200730162034800.png)

![image-20200730162354111](0730%20%EC%88%98%EC%97%85.assets/image-20200730162354111.png)



-- pdf 자료 참고 --













