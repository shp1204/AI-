# 1. 인공신경망

* 손실함수(Loss Function)

  : 손실 함수를 최소화하는 것, SGD 등의 알고리즘 사용

# 2. 모델 학습

Gradient Descent가 최소화 되도록 학습을 반복, 최소화한 곳의 parameter를 사용

![image-20200722142601361](0722%20%EC%88%98%EC%97%85.assets/image-20200722142601361.png)

* learning rate

![image-20200722142742253](0722%20%EC%88%98%EC%97%85.assets/image-20200722142742253.png)

![image-20200722142755498](0722%20%EC%88%98%EC%97%85.assets/image-20200722142755498.png)

* back propagation

![image-20200722143717143](0722%20%EC%88%98%EC%97%85.assets/image-20200722143717143.png)

편미분

![image-20200722143822821](0722%20%EC%88%98%EC%97%85.assets/image-20200722143822821.png)

--------

![image-20200722152356389](0722%20%EC%88%98%EC%97%85.assets/image-20200722152356389.png)

![image-20200722152545064](0722%20%EC%88%98%EC%97%85.assets/image-20200722152545064.png)

=> sigmoid 대신 relu 사용

![image-20200722152702228](0722%20%EC%88%98%EC%97%85.assets/image-20200722152702228.png)

Naive 방법 : 가중치를 표준 정규분포를 이용해 초기화, sigmoid 함수의 출력값이 0과 1에 치우치는 현상으로 기울기 소실 문제 발생

=> 표준 편차를 0.01로 하는 정규분포로 초기화

=> but sigmoid 대신 tanh 사용시 거의다 0값

=> Xavier 초기화 방법

표준 정규분포를 입력 개수의 제곱근으로 나누어줌

=> sigmoid일때는 괜츈한데, relu function 사용 시 xavier 초기화는 또다시 0값이 많아진다

=> He 초기화 방법

relu function시 사용

![image-20200722153951142](0722%20%EC%88%98%EC%97%85.assets/image-20200722153951142.png)

# 3. 최적화 알고리즘

1 ) SGD : 손실함수를 계산할 때 전체 training set을 사용하는 것을 Batch Gradient Descent라고 한다

- 계산량이 많아지는 것 방지
- 다소 부정확하지만 계산 속도가 빠르므로 더 많은 step, 반복으로 결국 비슷한 결과로 수렴

2 ) Momentum : 현재 gradient를 통해 이동하는 방향과는 별개로, 과거에 이동했던 방식을 기억하며 그 방향으로 일정 정도를 추가적으로 이동하는 방식

3 ) AdaGad(Adaptive Gradient)

: 많이 변화하지 않은 변수들은 step size를 크게하고, 많이 변화했던 변수들은 step size를 작게한다. => 변화를 많이 했으면 optimum 에 가까이 있을 확률이 높으므로 작은 크기로 이동하며 세밀하게 조절

![image-20200722155246746](0722%20%EC%88%98%EC%97%85.assets/image-20200722155246746.png)

학습을 계속 진행하면 step size가 너무 줄어드는 문제가 발생한다

4 ) RMS Prop

Adagrad 단점 해결을 위해 합을 지수평균으로 대체

![image-20200722155443357](0722%20%EC%88%98%EC%97%85.assets/image-20200722155443357.png)

5 ) Adam (Adaptive Moment Estimation)

: Momentum과 RMSProp을 합친 알고리즘, 지금까지 계산해온 기울기의 지수평균을 저장하고 RMSProp과 유사하게 기울기의 제곱값의 지수평균을 저장

![image-20200722155517172](0722%20%EC%88%98%EC%97%85.assets/image-20200722155517172.png)



# 4. 과적합 방지 기술

1 ) 정규화

![image-20200722160131994](0722%20%EC%88%98%EC%97%85.assets/image-20200722160131994.png)

2 ) L1, L2 정규화

![image-20200722160155405](0722%20%EC%88%98%EC%97%85.assets/image-20200722160155405.png)

3 ) Dropout

일정 비율의 뉴런을 임의로 drop 시켜 나머지 뉴런들만 학습하는 방법

보통 비율이 은닉층은 50%, 입력층은 26%가 일반적.(입력층은 parameter가 적기 때문에 더 적게)

역전파는 ReLU처럼 동작 / 순전파 때 신호를 통과시킨 뉴런은 역전파 때도 통과시키고, drop된 뉴런은 역전파 때도 신호를 차단

test는 모든 뉴런에 신호 전달. train 에서 사용했던 dropout 모델 사용하면 안된다 !



# 5. Data Augmentation

1 ) data가 부족할 때 늘리기 위해서 사용

2  ) 이미지 데이터에 많이 사용

: 평행이동, 좌우대칭, 랜덤 크랍, 밝기 조절, 크기 변경, 일부 지우기, 블러, 컬러 노이즈, 랜덤 노이즈