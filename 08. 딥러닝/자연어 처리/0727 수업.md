자연어 처리를 위한 딥러닝

고려대 석박통합 - 이준걸 강사님

http://www.datamarket.kr/xe/index.php?mid=board_pdzw77&document_srl=50368&m=0

* 청원글 중 중복된 것들은 모아두기
* 쓸데 없는 글 (기타 팝니다 -> 카테고리 : ㄱㅣ타 ㅋㅋ 노답)

---

# 1. NLP 사용

## A. 감성 분석

1 ) 긍 부정 분류

2 ) 평점 예측 -> regression task로 많이 하는데 support( 확률변수가 나올 수 있는 값의 범위 )가 minus값이 나올수도 있고, 5이상의 값이 나올 수도 있다 

ranking classification으로 가능

## B. 맞춤법 검사

매우 어려움

## C. 번역, 질의응답

챗봇

## D. 음성인식 스피커

음성인식 + test -> mulit



# 2. NLP Process

data collection -> tokenizing -> embedding -> similarity -> modeling

## tokenizing

* NO-FREE-LUNCH TEHOREM, https://ml-dnn.tistory.com/1

## embedding

- NNLM
- Word2Vec
- FastText
- BERT

## Similarity

## Modeling



# 3. EMBEDDING ( Count-based Representations )

## 1 ) Text Preprocessing

특수문자, 숫자, 조사, 접속사 등 제거

![image-20200727144955360](0727%20%EC%88%98%EC%97%85.assets/image-20200727144955360.png)

Stop Wording

-

성능이 개선됨 !!!!

## 2 ) Document Representation

## 3 ) Bag-of-words

* term-frequency(TDM) : Binary TDM과 Frequency TDM이 있는데 주로 Frequency TDM을 사용한다

  ![image-20200727150810366](0727%20%EC%88%98%EC%97%85.assets/image-20200727150810366.png)

  ![image-20200727150907453](0727%20%EC%88%98%EC%97%85.assets/image-20200727150907453.png)

  해석하기가 쉽다

  텍스트의 내용이 Word Frequency에 의해 표현됨

  임베딩 결과가 word 순서를 고려하지 않은 채 표현된다

  임베딩 결과만 놓고 봤을 때 raw text를 정확히 알 수 없다

  ![image-20200727151853491](0727%20%EC%88%98%EC%97%85.assets/image-20200727151853491.png)

*  Term Frequency – Inverse Document Frequency (TFIDF)

![image-20200727152235049](0727%20%EC%88%98%EC%97%85.assets/image-20200727152235049.png)

![image-20200727152353654](0727%20%EC%88%98%EC%97%85.assets/image-20200727152353654.png)

![image-20200727152743999](0727%20%EC%88%98%EC%97%85.assets/image-20200727152743999.png)

![image-20200727153046077](0727%20%EC%88%98%EC%97%85.assets/image-20200727153046077.png)

![image-20200727153543786](0727%20%EC%88%98%EC%97%85.assets/image-20200727153543786.png)

![image-20200727153759056](0727%20%EC%88%98%EC%97%85.assets/image-20200727153759056.png)

## 4 ) N-Gram 

잘 안쓴다

![image-20200727154328321](0727%20%EC%88%98%EC%97%85.assets/image-20200727154328321.png)



# 3. Document Similarity

![image-20200727171823900](0727%20%EC%88%98%EC%97%85.assets/image-20200727171823900.png)

코사인 유사도 사용

![image-20200727171951846](0727%20%EC%88%98%EC%97%85.assets/image-20200727171951846.png)

Jaccard Similarity 가 가장 많이 쓰임 (앙상블 느낌) -> Overlap Similarity

![image-20200727172236998](0727%20%EC%88%98%EC%97%85.assets/image-20200727172236998.png)

![image-20200727172250676](0727%20%EC%88%98%EC%97%85.assets/image-20200727172250676.png)

=> 너무 안좋아서 진짜 사용안함

![image-20200727172348499](0727%20%EC%88%98%EC%97%85.assets/image-20200727172348499.png)

=> 두개 동시에 등장하지 않는 것을 고려하지 않음

![image-20200727172429242](0727%20%EC%88%98%EC%97%85.assets/image-20200727172429242.png)

=> 분자를 불려줌

![image-20200727172452655](0727%20%EC%88%98%EC%97%85.assets/image-20200727172452655.png)

=> 매우 중요! 가장 쉽고 powerful 함

=> 단점 : 값이 매우 작아짐

=> 이를 해결하기 위해 필요없는 feature를 미리 자른다

![image-20200727172509478](0727%20%EC%88%98%EC%97%85.assets/image-20200727172509478.png)

=> 많이 사용

![image-20200727172549658](0727%20%EC%88%98%EC%97%85.assets/image-20200727172549658.png)

=> 코사인 유사도 ( -1 ~ 1 의 범위 )