# 1. DL- based Representations

*  Word Embedding

![image-20200728140628511](0728%20%EC%88%98%EC%97%85.assets/image-20200728140628511.png)

TDM matrix와 일치함

코사인 유사도로 두 단어의 유사도를 구하면 0이 나온다.



이 문제를 해결하기 위해 ? 

![image-20200728140810905](0728%20%EC%88%98%EC%97%85.assets/image-20200728140810905.png)

----

머신러닝 : parameter 및 feature를 사람이 조절해야 한다.

딥러닝 : parameter 및 feature를 모델이 스스로 찾고 선택한다. => 자동화 가능

-------

![image-20200728141546833](0728%20%EC%88%98%EC%97%85.assets/image-20200728141546833.png)



## 1 ) Neural Network Language Model (NNLM)

* Markov Assumption
* NNLM : 3 - gram일 때

![image-20200728142431720](0728%20%EC%88%98%EC%97%85.assets/image-20200728142431720.png)



Embedding matrix = look-up table 

 look-up table in C는 랜덤하게 준다

```
WC : Word, C(랜덤하게 주는 table)
v : vocab size, embedding 차원(one-hot encoding)
m : Embedding size (hyper parameter, 차원)

c는 우리는 모름 !!!
```

![image-20200728143146841](0728%20%EC%88%98%EC%97%85.assets/image-20200728143146841.png)



![image-20200728144506954](0728%20%EC%88%98%EC%97%85.assets/image-20200728144506954.png)

![image-20200728144531212](0728%20%EC%88%98%EC%97%85.assets/image-20200728144531212.png)

x = flatten(WC) 가 input이 된다.

여기에 bias를 더하고, tanh를 취하면서 hidden layer를 생성

![image-20200728144755955](0728%20%EC%88%98%EC%97%85.assets/image-20200728144755955.png)

U tanh(d +Hx) 에 softmax를 취한 뒤, classification을 한다. 

=> 비슷한 문맥을 가진 단어는 비슷한 벡터를 같게 됨.



## 2 ) Word2Vec

CBOW & Skip-gram VS N-gram

* CBOW : input으로 output 예측(context로 center 예측)
* Skip-gram : out put으로 input예측( center로 context 예측, input과 output dimension이 같다 ( auto-encoder ) )

activation function이 없으므로 비정형 데이터를 사용하지 않음

* skip-gram은 하나씩 다 해줌
* ![image-20200728152824521](0728%20%EC%88%98%EC%97%85.assets/image-20200728152824521.png)

```
input : [ 1 x 1000 ]

hidden layer에서 100차원으로 한다면 ?
[ 1 x 1000 ] * [ 1000 x 100 ]  = [ 1 x 100 ] 

얘를 다시 1000개의 output으로 바꿔주자
[ 1 x 100 ] * [ 100 x 1000 ] = [ 1 x 1000 ]

output : [ 1 x 1000 ]

위에서 사용된 [1000 x 100]과 [100 x 1000]은 transpose의 관계, 이 matrix는 랜덤하게 주어짐
```

input값은 one-hot encoding된 값 

-> 얼마나 많이 단어가 등장했는지, 등장하지 않았는지 등을 알기 위해서 '단어등장빈도'에 대한 정보를 얹어준다 

-> 이 정보를 가진 output을 도출 !!

## 3 ) Fasttext

![image-20200728153655809](0728%20%EC%88%98%EC%97%85.assets/image-20200728153655809.png)



## 4 ) Doc2Vec

![image-20200728154239462](0728%20%EC%88%98%EC%97%85.assets/image-20200728154239462.png)

```
100개의 doc와 1000개의 vocab이 있다면 [ 1100 x 100 ] 가 만들어진다 ( document index + 단어 )
output을 구한 뒤, document_index에 해당하는 부분의 값만 가져와서 doc간 유사도를 알 수 있다.
```

![image-20200728154250034](0728%20%EC%88%98%EC%97%85.assets/image-20200728154250034.png)

```
doc embedding을 다 한 뒤에 word embedding 사용
```

* 실습 

https://drive.google.com/drive/folders/1eglH4rzzaQTVsdqzDDArAuDrwgpePgcu



# 2.  Document Classification

![image-20200728165115478](0728%20%EC%88%98%EC%97%85.assets/image-20200728165115478.png)

![image-20200728165125239](0728%20%EC%88%98%EC%97%85.assets/image-20200728165125239.png)

긍정일 때 '단어'의 확률, 부정일 때 '단어'의 확률을 counting한다

![image-20200728165447494](0728%20%EC%88%98%EC%97%85.assets/image-20200728165447494.png)

![image-20200728165459166](0728%20%EC%88%98%EC%97%85.assets/image-20200728165459166.png)



# 3. RNN-based

## 1 ) MLE with text

![image-20200728165730707](0728%20%EC%88%98%EC%97%85.assets/image-20200728165730707.png)

![image-20200728165742671](0728%20%EC%88%98%EC%97%85.assets/image-20200728165742671.png)

벡터의 행, 단어 벡터의 개수가 신경망 입력 노드 개수와 맞지 않을 수 있다.

![image-20200728170047193](0728%20%EC%88%98%EC%97%85.assets/image-20200728170047193.png)

## 2 ) Recurrent Neural Network

단순한 MLP로 텍스트 데이터를 처리하기에 역부족임

=> RNN은 timestamp를 고려한다 ( 문장 또한 단어들이 이어져있기 때문에 sequential한 데이터를 다루기 위해 사용 )

![image-20200728170325121](0728%20%EC%88%98%EC%97%85.assets/image-20200728170325121.png)

![image-20200728170644379](0728%20%EC%88%98%EC%97%85.assets/image-20200728170644379.png)

![image-20200728170809360](0728%20%EC%88%98%EC%97%85.assets/image-20200728170809360.png)

one to one 은 거의 안 씀

```
input : word 여러개, timestamp로 연속되서 들어옴 
X, H에 weight가 있어야 한다.
```

many to many : machine translation

```
한국어 3개 -> 영어 3개
```

## 3 ) RNN

![image-20200728171301412](0728%20%EC%88%98%EC%97%85.assets/image-20200728171301412.png)

```
h t-1 : 이전의 정보 ( 벡터 )
x t : 새로 들어온 정보

위의 둘 다에 가중치를 준다

bias

activation function : tanh

output : h t 를 다음 input에 사용한다
```

* 모든 식마다 Wh와 Wx를 바꾼다면 timestamp를 고려하지 않은 것이 된다. 따라서 이 w값들을 share하게 된다. 이를 parameter sharing이라고 한다.

## 4 ) RNN Backword ( BPTT )

역 시간 순으로 back propagation이 일어난다

![image-20200728171702376](0728%20%EC%88%98%EC%97%85.assets/image-20200728171702376.png)

layer가 많으면 가장 끝의 loss 정보와 처음의 정보가 많이 다름 ( 손실되기 때문에 ) RNN 또한 그렇다. 그래서 gradient가 전달이 잘 안됨.

![image-20200728171903154](0728%20%EC%88%98%EC%97%85.assets/image-20200728171903154.png)

BPTT를 하면서 W가 이미 손실된 채로 처음에 오게 된다. 근데 얘를 모든 셀에 sharing하다보니 전체적으로 성능이 안좋아짐.

![image-20200728172107550](0728%20%EC%88%98%EC%97%85.assets/image-20200728172107550.png)

이를 해결하기 위해서,

1 ) GoogleNet :  n개씩 하고 다시 weight를 줌

![image-20200728172148929](0728%20%EC%88%98%EC%97%85.assets/image-20200728172148929.png)

2 ) ResNet : feature를 더해줌

![image-20200728172219693](0728%20%EC%88%98%EC%97%85.assets/image-20200728172219693.png)

다른 모델에서 보통 sigmoid도 vanishing문제가 발생하는데 이를 해결하기 위해서 relu등이 생기게 됨

그렇다면 RNN의 단점을 보완하고자 만들어진 모델은 ?

## 5 ) LSTM

RNN보다 parameter보다 4개 더 많아서 무거움

-> 현재 GRU 사용 : https://yjjo.tistory.com/18 참고

-------

이후는 영상강의 참조 !

 ## 6 ) CNN

## 7 ) Character-CNN

