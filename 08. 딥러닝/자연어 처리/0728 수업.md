# 1. DL- based Representations

*  Word Embedding

![image-20200728140628511](0728%20%EC%88%98%EC%97%85.assets/image-20200728140628511.png)

TDM matrix와 일치함

코사인 유사도로 두 단어의 유사도를 구하면 0이 나온다.



이 문제를 해결하기 위해 ? 

![image-20200728140810905](0728%20%EC%88%98%EC%97%85.assets/image-20200728140810905.png)

----

머신러닝 : parameter 및 feature를 사람이 조절해야 한다.

딥러닝 : parameter 및 feature를 모델이 스스로 찾고 선택한다. => 자동화 가능

-------

![image-20200728141546833](0728%20%EC%88%98%EC%97%85.assets/image-20200728141546833.png)



## 1 ) Neural Network Language Model (NNLM)

* Markov Assumption
* NNLM : 3 - gram일 때

![image-20200728142431720](0728%20%EC%88%98%EC%97%85.assets/image-20200728142431720.png)



Embedding matrix = look-up table 

 look-up table in C는 랜덤하게 준다

```
WC : Word, C(랜덤하게 주는 table)
v : vocab size, embedding 차원(one-hot encoding)
m : Embedding size (hyper parameter, 차원)

c는 우리는 모름 !!!
```

![image-20200728143146841](0728%20%EC%88%98%EC%97%85.assets/image-20200728143146841.png)



![image-20200728144506954](0728%20%EC%88%98%EC%97%85.assets/image-20200728144506954.png)

![image-20200728144531212](0728%20%EC%88%98%EC%97%85.assets/image-20200728144531212.png)

x = flatten(WC) 가 input이 된다.

여기에 bias를 더하고, tanh를 취하면서 hidden layer를 생성

![image-20200728144755955](0728%20%EC%88%98%EC%97%85.assets/image-20200728144755955.png)

U tanh(d +Hx) 에 softmax를 취한 뒤, classification을 한다. 

=> 비슷한 문맥을 가진 단어는 비슷한 벡터를 같게 됨.



## 2 ) Word2Vec

CBOW & Skip-gram VS N-gram

* CBOW : input으로 output 예측(context로 center 예측)
* Skip-gram : out put으로 input예측( center로 context 예측, input과 output dimension이 같다 ( auto-encoder ) )

activation function이 없으므로 비정형 데이터를 사용하지 않음

* skip-gram은 하나씩 다 해줌
* ![image-20200728152824521](0728%20%EC%88%98%EC%97%85.assets/image-20200728152824521.png)

```
input : [ 1 x 1000 ]

hidden layer에서 100차원으로 한다면 ?
[ 1 x 1000 ] * [ 1000 x 100 ]  = [ 1 x 100 ] 

얘를 다시 1000개의 output으로 바꿔주자
[ 1 x 100 ] * [ 100 x 1000 ] = [ 1 x 1000 ]

output : [ 1 x 1000 ]

위에서 사용된 [1000 x 100]과 [100 x 1000]은 transpose의 관계, 이 matrix는 랜덤하게 주어짐
```

input값은 one-hot encoding된 값 

-> 얼마나 많이 단어가 등장했는지, 등장하지 않았는지 등을 알기 위해서 '단어등장빈도'에 대한 정보를 얹어준다 

-> 이 정보를 가진 output을 도출 !!

## 3 ) Fasttext

![image-20200728153655809](0728%20%EC%88%98%EC%97%85.assets/image-20200728153655809.png)



## 4 ) Doc2Vec

![image-20200728154239462](0728%20%EC%88%98%EC%97%85.assets/image-20200728154239462.png)

```
100개의 doc와 1000개의 vocab이 있다면 [ 1100 x 100 ] 가 만들어진다 ( document index + 단어 )
output을 구한 뒤, document_index에 해당하는 부분의 값만 가져와서 doc간 유사도를 알 수 있다.
```

![image-20200728154250034](0728%20%EC%88%98%EC%97%85.assets/image-20200728154250034.png)

```
doc embedding을 다 한 뒤에 word embedding 사용
```



https://drive.google.com/drive/folders/1eglH4rzzaQTVsdqzDDArAuDrwgpePgcu

