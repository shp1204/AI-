# 1. KNN

한 점 주변의 K개의 점들의 색을 보고, 현재 점을 분류한다.

K개의 점과 현재 점의 거리는 어떻게 정할 것인가 ?

: distance function 을 정한다.

![image-20200714140831455](0714%20%EC%88%98%EC%97%85.assets/image-20200714140831455.png)

1 ) k 결정

다수결(majority class)로 새로운 데이터에 대한 클래스 결정

K  가 점점 커진다면 ? 점점 smoothy 해진다. 따라서 label noise를 줄여준다. 

![image-20200714141114045](0714%20%EC%88%98%EC%97%85.assets/image-20200714141114045.png)

2 ) distance function

![image-20200714141238407](0714%20%EC%88%98%EC%97%85.assets/image-20200714141238407.png)

![image-20200714141309433](0714%20%EC%88%98%EC%97%85.assets/image-20200714141309433.png)

이 식에서 p가 1일때 manhatton, p가 2일 때 euclidean.

이외에도 다양한 p값을 넣은 measure가 있지만 보통 이 두가지 사용한다.

* manhatton distance(L1-norm)
* euclidean distance(L2-norm)

![image-20200714141629330](0714%20%EC%88%98%EC%97%85.assets/image-20200714141629330.png)

이 두가지를 계산하는데 거리가 가까우면 w(x)값이 커야 한다. 따라서 다음의 함수를 사용하여 역수를 취해준다.

![image-20200714141727684](0714%20%EC%88%98%EC%97%85.assets/image-20200714141727684.png)



[ 장점 ]

학습을 하지 않는다. => 이게 왜 장점일까 ? => training step이 있다는 것은 시간이 걸린다는 의미이므로 시간을 들이지 않고 예측할 수 있다.

[ 단점 ]

컴퓨터는 내 주변의 k가 무엇인지 모르기 때문에 모든 점들에 대한 distance값을 계산해주어야한다. 따라서 시간이 많이 걸린다

불균형 데이터가 있다면, 다수결에 의한 방법이기 때문에 minor한 값들은 분류하기 어렵다.

k가 작을 때는 overfitting이 많이 일어난다.

k가 클 때는 underfitting이 많이 일어난다.



# 2. NBC : Naive Bayse Classifier

## 1 ) Bayse 이론

: precision, recall과 다름. 구분 !

![image-20200714145148883](0714%20%EC%88%98%EC%97%85.assets/image-20200714145148883.png)

확률처럼 생겼는데 확률이 아니라 Likelihood ? !!

**Likelihood VS Probability 비교 중요** 

bayesian : 내 경험에 기반해서 내는 확률

frequentist : 객관적인 확률

```
ex ) 가위바위보, 의사가 환자의 병 진단
probability : 이상, likelihood : 현실

probability : 내가 이미 알고 있는 data가 나타날 확률
likelihood : 데이터를 기반으로 분포의 모수를 추정하는 것
```



## 2 ) naive bayse 의 가정 : conditionally independent

![image-20200714150404734](0714%20%EC%88%98%EC%97%85.assets/image-20200714150404734.png)

사건 c가 일어났을 때, 사건 A와 B가 동시에 일어날 확률

![image-20200714150610864](0714%20%EC%88%98%EC%97%85.assets/image-20200714150610864.png)

![image-20200714150812640](0714%20%EC%88%98%EC%97%85.assets/image-20200714150812640.png)

이 때, P(class=Good | X)와 P(class=Bad | X)를 비교해서 더 큰 값을 가지는 클래스로 분류!

## 3 ) 예시 : spam

## 4 ) zero probability

한 값만 0이어도 값 전체가 0으로 바뀐다.

따라서, smoothing을 해주자

![image-20200714152546874](0714%20%EC%88%98%EC%97%85.assets/image-20200714152546874.png)

* Laplacian Correction : 그냥 현 데이터에서 1씩 더하는 것이다.
* how to avoid ~?

[ 장점 ]

확률의 곱이기 때문에 구현 또한 간단하다

spam 처럼 현실 세계에도 적용된다면 좋을 듯

[ 단점 ]

spam은 괜찮지만, 현실 문제들은 feature들 간의 dependency가 있기 때문에 잘 동작을 하지 않는다는 것이 단점이다.



# 3. Support Vector Machine 

## 1 ) support vector 

: decision boundary와 가까운 data points

나머지 example은 무시해도 되기 때문에 computation을 줄일 수 있다는 것이 장점이다 !

## 2 ) Non-Linear SVM

이럴 때는 선형 분리가 불가능하기 때문에 Kernel Trick을 사용한다

![image-20200714162321870](0714%20%EC%88%98%EC%97%85.assets/image-20200714162321870.png)

이 때 y = x **2 의 커널을 사용하여 분리할 수 있따

![image-20200714162406594](0714%20%EC%88%98%EC%97%85.assets/image-20200714162406594.png)

![image-20200714162423069](0714%20%EC%88%98%EC%97%85.assets/image-20200714162423069.png)

다양한 함수로 커널 함수를 정의할 수 있다

## 3 ) Linear-SVM

![image-20200714163023218](0714%20%EC%88%98%EC%97%85.assets/image-20200714163023218.png)

![image-20200714163118280](0714%20%EC%88%98%EC%97%85.assets/image-20200714163118280.png)

X+ - X- 에서 람다와 w는 어디에서 등장한걸까 ? 

람다는 

wtax - wtbx = 0이기 때문에

wt(a - b) = 0이다 

a-b = 0이기 때문에 wt는 방향을 의미한다

--- margin은 maximize하였으니 이제 분류를 하자

![image-20200714163952061](0714%20%EC%88%98%EC%97%85.assets/image-20200714163952061.png)

 정리하면![image-20200714164018594](0714%20%EC%88%98%EC%97%85.assets/image-20200714164018594.png)

여전히 계산이 복잡하므로

![image-20200714164052369](0714%20%EC%88%98%EC%97%85.assets/image-20200714164052369.png)

정리하면

![image-20200714164150816](0714%20%EC%88%98%EC%97%85.assets/image-20200714164150816.png)

최종적으로 이 두 식도 합하여주면

![image-20200714164209113](0714%20%EC%88%98%EC%97%85.assets/image-20200714164209113.png)

---

* 목적함수 증명 중

* 나중에 w,b를 구하기 위해서는 현재 식에 있는 w, b를 없애주어야한다

  ![image-20200714165544704](0714%20%EC%88%98%EC%97%85.assets/image-20200714165544704.png)

w의 t를 취하면 a, y, x중 벡터인 x만 t가 생김

![image-20200714165601587](0714%20%EC%88%98%EC%97%85.assets/image-20200714165601587.png)

![image-20200714165619180](0714%20%EC%88%98%EC%97%85.assets/image-20200714165619180.png)

1, 2의 식을 계산하면

![image-20200714165457357](0714%20%EC%88%98%EC%97%85.assets/image-20200714165457357.png)

![image-20200714170200578](0714%20%EC%88%98%EC%97%85.assets/image-20200714170200578.png)