# 1. Logistic Regression

## 1 ) Sigmoid Function

* 확률이 0.5 이상 / 이하인 경우로 학습을 시킨다

![image-20200713143717775](0713%20%EC%88%98%EC%97%85.assets/image-20200713143717775.png) 

ex 1 ) linear

![image-20200713143817138](0713%20%EC%88%98%EC%97%85.assets/image-20200713143817138.png)

ex 2 ) non-linear

![image-20200713143947708](0713%20%EC%88%98%EC%97%85.assets/image-20200713143947708.png)

## 2 ) Cost Function

정답이 틀렸을 때 벌을 크게 줌으로써 식을 유도한다.

![image-20200713150649718](0713%20%EC%88%98%EC%97%85.assets/image-20200713150649718.png)

실제가 0일 때와 1일 때를 분류한 뒤, 이 두 개의 식을 합쳐준다.

정리하면

![image-20200713150744541](0713%20%EC%88%98%EC%97%85.assets/image-20200713150744541.png)

: y가 0일 때는 뒤의 식만 남고, y가 1일 때는 (1-y) 때문에 앞의 식만 남는다.

## 3 ) Gradient Descent

![image-20200713151028151](0713%20%EC%88%98%EC%97%85.assets/image-20200713151028151.png)

# 2. Softmax Regression

: multi-class classification 

![image-20200713151509606](0713%20%EC%88%98%EC%97%85.assets/image-20200713151509606.png)

나온 결과 값을 확률로 표현하고, 정확히 분류하기 위해서 one-hot encoding한다.



* softmax 구현 시 주의할 점 !

  지수함수 사용 시 값이 매우 커지는데 over-flow를 방지하기 위해서

  ```python
  def revised_softmax(x):
  	e_x = np.exp(x – np.max(x))
  	return e_x/np.sum(e_x)
  ```

* Cost Function : Cross Entropy

  ![image-20200713151555241](0713%20%EC%88%98%EC%97%85.assets/image-20200713151555241.png)

# 3. Evaluation Metrics

## 1 ) Confusion Matrix

![image-20200713162659426](0713%20%EC%88%98%EC%97%85.assets/image-20200713162659426.png)

* 정확도

  ![image-20200713162909563](0713%20%EC%88%98%EC%97%85.assets/image-20200713162909563.png)

![image-20200713163236097](0713%20%EC%88%98%EC%97%85.assets/image-20200713163236097.png)

정확도 : 3 / 5

왜 정확도를 계속 사용하지 못할까 ? => class가 불균형한 경우가 있기 때문에 !

* 정밀도 (precision) : 모델의 입장
* ![image-20200713163708935](0713%20%EC%88%98%EC%97%85.assets/image-20200713163708935.png)

![image-20200713163539679](0713%20%EC%88%98%EC%97%85.assets/image-20200713163539679.png)

: 실제로 예측을 한 것 중에서 정답인 것

* 재현율 (recall) : 데이터의 입장

![image-20200713163639783](0713%20%EC%88%98%EC%97%85.assets/image-20200713163639783.png)

![image-20200713163720347](0713%20%EC%88%98%EC%97%85.assets/image-20200713163720347.png)

: 정답인 것 중에 예측한 것

* Precision-Recall Cuve ( PR-Curve )

X축을 Recall, Y축을 Precision으로 하여 시각화한 그래프

보통 데이터가 불균형될 때 사용한다.

![image-20200713165135432](0713%20%EC%88%98%EC%97%85.assets/image-20200713165135432.png)

## 2 ) F-measure

precision, recall의 조화평균

![image-20200713165343274](0713%20%EC%88%98%EC%97%85.assets/image-20200713165343274.png)

왜 조화 평균을 사용할까 ?

: 둘 중 하나의 값이 너무 작을 때 작은 값에 가깝게 해주기 위해서 ( 단순히 중간값 x )

데이터가 불균형하다면 평균을 보고 데이터의 가치를 믿을 수 없기 때문에 

![image-20200713170458334](0713%20%EC%88%98%EC%97%85.assets/image-20200713170458334.png)

![image-20200713170914399](0713%20%EC%88%98%EC%97%85.assets/image-20200713170914399.png)

모든 쿼리에 대해 평균을 내기위해서 계산

![image-20200713170934411](0713%20%EC%88%98%EC%97%85.assets/image-20200713170934411.png)