bias : 목표치에 얼마만큼 벗어났는가 ?

variance : 추정값이 얼마만큼 퍼졌는가 ?

high bias, low variance : underfitting

low bias, high variance : overfitting

------

# 1. 클러스터링

: 분류 문제와 유사하다고 볼 수 있지만 label이 없기 때문에 다르다고 할 수 있다.

## 1 ) 타당성 지표

```
군집간 거리 : 군집 간 거리가 멀어야 좋음
군집의 지름 : 군집의 지름이 작아야 좋음
-> 지름이 클수록 뭉개진다
군집의 분산 : 군집의 분산이 작아야 좋음
-> 너무 퍼져있으면 같은 군집이라고 볼 수 있을지 의문이 든다. 

Dunn Index : 클러스터 간 거리를 최소화하고(분자), 군집 내 요소 간 거리를 최대화하여(분모) 계산하는 지표
```

* 가장 많이 쓰이는 것 : 군집간의 거리와, 군집 내부의 거리는 작게하는 것

* centroid : 각 클러스터를 가장 잘 나타내는 대표점 (무게중심, 중간값 등등)



# 2. K-MEANS

가장 쉽고 빠르게 할 수 있는 군집화 모델

* 군집화 모델을 사용하고자 한다면 ?

EDA 과정을 통해 모델을 결정할 때, 사용가능한 모델이 정말 많은데, 무거운 모델을 계획하기 전에 간단한 intuition을 얻기 위해 돌리는 모델

* k-means는 EM알고리즘 기반

```
입력 : 클러스터 수(K), 트레이닝 셋

알고리즘 :  K개의 무게중심을 랜덤하게 초기화한 뒤 class를 매긴다. 이후 각클래스애 해당한다고 생각하는 centroid를 옮겨가면서 반복한다.
```

* loss function : 각 클러스터의 centroid의 분산의 평균

  loss function을 최소화 해야한다. -> 각 클러스터의 분산이 작다 -> 각 클러스터의 중심과 각 점과의 거리 길이가 작다

* 하지만 한번에 최소 값을 찾기 힘들기 때문에 군집화한 종류를 여러개 두고, 이를 다 돌면서 최적화한다 (for문 두개 돌리는 느낌)

![image-20200716144238430](0716%20%EC%88%98%EC%97%85.assets/image-20200716144238430.png)

## 단점

* 군집의 크기나 밀도가 다를 경우에 의도치 않은 결과가 나올 수 있다

![image-20200716144614692](0716%20%EC%88%98%EC%97%85.assets/image-20200716144614692.png)

=> 우리가 처음에 centroid를 찍어줄 수 있을까 ?

* 데이터 분포가 특이한 경우에도 원하는 결과가 나오지 않을 수 있다

![image-20200716144625493](0716%20%EC%88%98%EC%97%85.assets/image-20200716144625493.png)

=> 주변의 밀도를 파악해보자

=> A점 근처에 어떤 아이들이 밀도있게 있는가 ?(15, 16년도)

## 클러스터 개수 정하기

: elbow method를 찍어본다



# 3. KNN

K : 주변에 몇개의 점을 볼 것인가

![image-20200716150310071](0716%20%EC%88%98%EC%97%85.assets/image-20200716150310071.png)

k가 너무 작으면 overfitting이 될 수 있다.

k가 너무 크면 underfitting이 많을 것이다.

## knn 거리 지표

1 ) 유클리드 거리(Euclidean distance)

2 ) 맨해튼 거리(Manhaten distance) : 거리의 절댓값을 더하였다

3 ) 마할라노비스 거리(Mahalanobis distance)

데이터들의 공분산을 고려하여 변형해준다

![image-20200716150746981](0716%20%EC%88%98%EC%97%85.assets/image-20200716150746981.png)

![image-20200716150818153](0716%20%EC%88%98%EC%97%85.assets/image-20200716150818153.png)

원을 보았을 때는 다른 거리여야할 것 같지만 사실 같은 거리이다.

일반적으로 유클리드 거리를 사용

# GMM(Gaussian Mixture Model)

k-means의 범용화된 알고리즘

* discriminative : y값 고정

  => K-means (1)

* generative : y값이 확률적

  => GMM (0.7, 0.2, 0.1)



![image-20200716151720432](0716%20%EC%88%98%EC%97%85.assets/image-20200716151720432.png)

다변수 정규분포(Multivariate Gaussian Distribution) 또한 가정할 수 있따.

## 동작 과정

임의로 3개, 각각 평균과 분산이 있음 . 따라서 6개의 파라미터를 조정할 수 있다

이를 반복하면서 클러스터를 구분

## 모수 추정

파이k : 각각의 클래스가 선택될 확률

## class 분류



# 계층 클러스터링

# 모델 별 비교

![image-20200716154230100](0716%20%EC%88%98%EC%97%85.assets/image-20200716154230100.png)



