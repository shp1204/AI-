* 정민수 강사님 : 서울대학원 데이터마이닝 연구실에서 석박통합하고 계심 / 그래프 / 실시간 데이터 생성 환경에서 빅데이터를 어떻게 처리할 것인지 연구 ?!

# 1. 의사결정트리( Decision Tree )

## 1 ) 목표

* Root node -> Internal Nodes -> Leaf nodes

* leaf node의 불순도를 낮추는 것이 목표이다.

  ----

* 심장병을 진단하는 기준이 세가지라고 하자.

* 목표는 leaf node에 심장병인 환자 / 아닌 환자 로 정확하게 분류하는 것이다.

  ---

  example : iris 를 분류해보자

  ![image-20200715140748789](0715%20%EC%88%98%EC%97%85.assets/image-20200715140748789.png)



## 2 ) 불순도 지표

보통 지니 계수를 사용한다.

* 지니 계수

  ![image-20200715140908025](0715%20%EC%88%98%EC%97%85.assets/image-20200715140908025.png)

: 순도가 높을 수록 값이 낮아진다

* Entropy
* ![image-20200715141132685](0715%20%EC%88%98%EC%97%85.assets/image-20200715141132685.png)
* Misclassification error
* ![image-20200715141141066](0715%20%EC%88%98%EC%97%85.assets/image-20200715141141066.png)

세 가지를 비교해보면 misclassification은 미분이 불가능한 시점이 존재하고, entropy는 log계산으로 복잡하기 때문에 보통 entropy 사용

![image-20200715141228945](0715%20%EC%88%98%EC%97%85.assets/image-20200715141228945.png)



## 3 ) 분류

### * 1 . chest pain

![image-20200715141439604](0715%20%EC%88%98%EC%97%85.assets/image-20200715141439604.png)

![image-20200715141451958](0715%20%EC%88%98%EC%97%85.assets/image-20200715141451958.png)

이 값들을 본 뒤 , 둘을 나누는 기준값을 구해야 한다. 이를 역으로 계산하면 기준값을 구할 수 있다.

![image-20200715141603517](0715%20%EC%88%98%EC%97%85.assets/image-20200715141603517.png)

=> 값이 떨어졌기 때문에 분류를 잘 했다고 할 수 있다.

### *Good Blood Circulation, Blocked Arteries의 분류 기준값도 계산한 뒤, 가장 낮은 값을 채택

이후에 지속적으로 낮은 값들을 선택한다

----

그렇다면 언제까지 분류할 것인가 ? 

분류해보았을 때 불순도가 낮아지면 더 분류하고, 아니면 추가적으로 분류하지 말자.

![image-20200715141934850](0715%20%EC%88%98%EC%97%85.assets/image-20200715141934850.png)

분류를 계속하다보면 다음과 같이 overfitting된 경우가 발생한다. 이 때는 오히려 성능을 떨어뜨리기 때문에 규제를 주어야 한다.

![image-20200715142132039](0715%20%EC%88%98%EC%97%85.assets/image-20200715142132039.png)

----

숫자가 아닐때는 ?

![image-20200715142416343](0715%20%EC%88%98%EC%97%85.assets/image-20200715142416343.png)

각 값들의 평균값을 기준으로 불순도를 다 구한 뒤, 비교해본다.

* 왜 굳이 2개로만 할까 ? 여러개 할 수 있지만 너무 복잡하다 !



## 4 ) 회귀

분류한 뒤 값들과 이 집단의 평균의 MSE를 구해보면서 상위 노드에서 기준 값을 조정한다.

![image-20200715143548793](0715%20%EC%88%98%EC%97%85.assets/image-20200715143548793.png)

? 가 들어가는 집단은 3 또는 8의 결과값을 뱉어내고, 이 값과의 오차를 계산할 수 있다.

또 다르게 1,3 // 5, 7, 9로 나눈다면 결과값은 2, 7이 되는데, 인풋 ?에 대한 아웃풋 값과의 오차를 다시 결정한다. 

오차들을 비교하여 더 적은 값의 모델을 선택한다.

## 특징

```
• 특징 값에 대한 제약이 적다.
• 범주형, 수치형, 혼합형
• 특징 전처리가 없다.
• 손실 데이터(missing data) 처리가 쉽다.
• 분류 결과가 해석가능하다.
=> 현실에서 어떤 오류가 발생하였을 때 분류 기준을 파악하기 용이하기 때문에 모델의 해석이 가능하다. 따라서 문제를 해결할 수 있다.
• 학습이 끝난 트리의 작업 속도가 매우 빠르다.
• 의사결정트리의 학습은 그리디(Greedy) 알고리즘이다. ( global optimer가 아니므로 항상 최적이 아님 )
```

보통 CART를 많이 사용한다.

![image-20200715143216462](0715%20%EC%88%98%EC%97%85.assets/image-20200715143216462.png)

--------

* Information gain : 분개 이후에 분개 이전과 비교하여 좋아진 값의 크기 ( impurity가 얼마나 낮아졌는가 )

![image-20200715144640220](0715%20%EC%88%98%EC%97%85.assets/image-20200715144640220.png)

* entropy

값이 한 쪽으로 몰릴 수록 그 정보의 가치가 낮아진다. 하지만 우리가 주목하는 것은 잘 일어나지 않는 부분이므로 entropy 값이 낮을수록 정보의 가치가 없고, 높을수록 class의 비율이 균일하므로 정보가 가치가 있다고 할 수 있다.

# 2. 앙상블 ( Ensemble )

## [ 1 ] Voting

### 1 ) hard voting

![image-20200715150242210](0715%20%EC%88%98%EC%97%85.assets/image-20200715150242210.png)

여러 모델을 돌려서 다수결에 의해 가장 많이 나온 class를 반환

### 2 ) soft voting 

: 각각의 값들의 확률을 구하여 전부를 조합 및 선택하여 결과로 낸다. 일반적으로 hard voting보다 많이 사용함

![image-20200715150334267](0715%20%EC%88%98%EC%97%85.assets/image-20200715150334267.png)

=> regression

![image-20200715150442884](0715%20%EC%88%98%EC%97%85.assets/image-20200715150442884.png)

### 3 ) bagging : bootstrapping + aggregating

* bootstrapping : 복원 추출 ( 보통 더 잘 쓰임 )

  ![image-20200715150823650](0715%20%EC%88%98%EC%97%85.assets/image-20200715150823650.png)

  ㅇㅣ 과정을 통해 각각의 fitting 하면 bias가 높아질 수 밖에 없다.  전체 데이터를 다시 학습시키면  bias는 기존과 유사하고, variance가 떨어진다.

* pasting : 비복원 추출

* OOB (Out of Bag) 평가

  ![image-20200715151501410](0715%20%EC%88%98%EC%97%85.assets/image-20200715151501410.png)

이 데이터들을 validation에 활용한다.

### 4 ) random patch, random subspace

![image-20200715151547258](0715%20%EC%88%98%EC%97%85.assets/image-20200715151547258.png)

## [2] Boosting

### 1 ) AdaBoost : 틀린 것에 대해 가중치를 주어서 틀린 것을 더 학습할 수 있도록 한다

![image-20200715151802559](0715%20%EC%88%98%EC%97%85.assets/image-20200715151802559.png)

![image-20200715151852003](0715%20%EC%88%98%EC%97%85.assets/image-20200715151852003.png)

### 2 ) Gradient Boost

![image-20200715152527447](0715%20%EC%88%98%EC%97%85.assets/image-20200715152527447.png)



# 3. 랜덤 포레스트

배깅 : 전체에 대해 복원 추출 + 학습 + 피팅

부스팅 : feature도 sampling

---------

decision tree를 각각 만들어서 이 결과로 랜덤 포레스트를 구축한다. decision tree 처럼 한번에 끝나지 않고 반복적으로 진행한다.

![image-20200715153453892](0715%20%EC%88%98%EC%97%85.assets/image-20200715153453892.png)

-----

missing data 처리

## OOD

![image-20200715153541065](0715%20%EC%88%98%EC%97%85.assets/image-20200715153541065.png)

처음에 Missing data를 채우기 위해

![image-20200715153558671](0715%20%EC%88%98%EC%97%85.assets/image-20200715153558671.png)![image-20200715153606979](0715%20%EC%88%98%EC%97%85.assets/image-20200715153606979.png)

다수결 또는평균으로 값을 채워 넣는다.

![image-20200715153626725](0715%20%EC%88%98%EC%97%85.assets/image-20200715153626725.png)