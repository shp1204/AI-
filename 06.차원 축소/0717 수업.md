# 1. 차원의 저주와 차원 축소

## 1 ) 차원의 저주

hyper cube : 모든 면이 정사각형이라고 상상..!

![image-20200717140747096](0717%20%EC%88%98%EC%97%85.assets/image-20200717140747096.png)

![image-20200717140803647](0717%20%EC%88%98%EC%97%85.assets/image-20200717140803647.png)

![image-20200717140809028](0717%20%EC%88%98%EC%97%85.assets/image-20200717140809028.png)

차원이 늘어나면 늘어날 수록 특정 범위안에 들어가는 점들이 희박해진다.

=> 이미 데이터들이 희박하게 존재하면 클러스터링 하기가 어렵다 / 넓은 범위의 데이터들을 한 군집으로 묶어야함

=> 성능이 좋지 않다.. !

=> 그래서 고차원일수록 데이터의 양이 많아져야한다.

![image-20200717141157963](0717%20%EC%88%98%EC%97%85.assets/image-20200717141157963.png)

EX )

16 * 16 = 256 인데 3 (RGB) => 256 * 3 = 768

이런 사진이 만장 있다고 해보자.

=> 너무 많아...!!

=> 픽셀 하나하나를 굳이 학습해야할까 ? ( observation space 사진 중에 보고자하는 부분은 얼마 안된다 !)

=> 중요한 부분의 픽셀 특징을 파악하여 ( 패턴을 파악 ) 분류 기준으로 삼는다 : latent space

* 언제 차원축소를 시행할까 ? 

  => 차원이 너무커서 분류기가 학습시키지 못할 때 차원축소를 한다. 



# 2. PCA : Principal Component Analysis

## 1 ) 고유벡터 : Eigen vector

벡터에 특정행렬을 곱하여도 방향이 변하지 않는 벡터( 크기는 변함 )

![image-20200717142529017](0717%20%EC%88%98%EC%97%85.assets/image-20200717142529017.png)

![image-20200717142708041](0717%20%EC%88%98%EC%97%85.assets/image-20200717142708041.png)

![image-20200717142755433](0717%20%EC%88%98%EC%97%85.assets/image-20200717142755433.png)

![image-20200717142729423](0717%20%EC%88%98%EC%97%85.assets/image-20200717142729423.png)

![image-20200717142743369](0717%20%EC%88%98%EC%97%85.assets/image-20200717142743369.png)

ex )

![image-20200717143021374](0717%20%EC%88%98%EC%97%85.assets/image-20200717143021374.png)

![image-20200717143122113](0717%20%EC%88%98%EC%97%85.assets/image-20200717143122113.png)



## 2 ) 대각화 (Diagonalization)

![image-20200717143239656](0717%20%EC%88%98%EC%97%85.assets/image-20200717143239656.png)

고유값들을 대각성분으로 갖는 행렬을 D, 고유값들에 대응하는 고유벡터들을 열벡터로 갖는 행렬을 Q라고 하며 , 위의 과정을 대각화 라고한다.

대칭 행렬은 항상 직교행렬로 대각화 가능



## 3 ) PCA

![image-20200717143438426](0717%20%EC%88%98%EC%97%85.assets/image-20200717143438426.png)

모든 데이터에서 각 행의 평균을 빼서 모든 행의 평균이 0이 되게 한다( Centering 작업 )

![image-20200717143448069](0717%20%EC%88%98%EC%97%85.assets/image-20200717143448069.png)

데이터의 분산을 최대로 하는 직선을 찾고자 한다.

b : error은 최소화, c : 최대화하자, a : 고정값

![image-20200717152416338](0717%20%EC%88%98%EC%97%85.assets/image-20200717152416338.png)

![image-20200717143735681](0717%20%EC%88%98%EC%97%85.assets/image-20200717143735681.png)

선을 돌려가면서 하다가 찾은 직선을 첫번째 주성분(pc1), 빨간색 벡터를 pc1의 싱귤러 벡터라고 한다. pc1의 싱귤러 벡터는 공분산 행렬의 가장 큰 고유값에 대한 eigen vector가 된다. 

![image-20200717143943990](0717%20%EC%88%98%EC%97%85.assets/image-20200717143943990.png)

한 번 끝남 !  또 해 보자 !



pc2는 pc1과 수직인 직선 중 정사영 했을 때의 분산이 가장 큰 직선, pc2의 싱귤러 벡터는 공분산 행렬의 두번째 큰 고유값에 대한 고유벡터

![image-20200717144126122](0717%20%EC%88%98%EC%97%85.assets/image-20200717144126122.png)



## 4 ) PC별 중요도 계산

PC1, PC2의 중요도를 계산한 뒤, Scree plot으로 나타낸다.



## 5 ) PCA 사용 TIPS 

* 이 때, feature 의 Scaling을 반드시 해주어야한다.

* 데이터의 평균을 0으로 바꾸는 작업을 꼭 해야한다.



## 6 ) PCA원리 설명

![image-20200717145821440](0717%20%EC%88%98%EC%97%85.assets/image-20200717145821440.png)

평균 E(x) 는 0으로 맞춰주는 centering을 했기 때문에 값이 0이된다.

![image-20200717145831193](0717%20%EC%88%98%EC%97%85.assets/image-20200717145831193.png)

결국 최적화 문제를 풀어야하는 셈이다.

이를 해결하기 위해 라그랑제 승수법을 이용하여 계산한다.

![image-20200717150001032](0717%20%EC%88%98%EC%97%85.assets/image-20200717150001032.png)

첫번째 항이 max가 되는 시점은 다음의 식에 의하여 e가 eigen vector가 되는 시점이다. ![image-20200717150024993](0717%20%EC%88%98%EC%97%85.assets/image-20200717150024993.png)

![image-20200717150317883](0717%20%EC%88%98%EC%97%85.assets/image-20200717150317883.png)

## 7 ) 데이터 압축과 복원

![image-20200717150646870](0717%20%EC%88%98%EC%97%85.assets/image-20200717150646870.png)

![image-20200717150656170](0717%20%EC%88%98%EC%97%85.assets/image-20200717150656170.png)



* SVD 더 공부해보기 => 차원축소가 나으하다 => recommendation시에 사용 가능

# 2. CUR  Decomposition

## 1 ) 목표

![image-20200717153514488](0717%20%EC%88%98%EC%97%85.assets/image-20200717153514488.png)

## 2 ) 행렬 C의 의미

![image-20200717153554618](0717%20%EC%88%98%EC%97%85.assets/image-20200717153554618.png)

![image-20200717153602619](0717%20%EC%88%98%EC%97%85.assets/image-20200717153602619.png)

![image-20200717153829511](0717%20%EC%88%98%EC%97%85.assets/image-20200717153829511.png)

비복원 추출이기 때문에 중복 샘플링이 될 수 있다

![image-20200717154008760](0717%20%EC%88%98%EC%97%85.assets/image-20200717154008760.png)

![image-20200717154040184](0717%20%EC%88%98%EC%97%85.assets/image-20200717154040184.png)

![image-20200717154141878](0717%20%EC%88%98%EC%97%85.assets/image-20200717154141878.png)

비중이 엄청 큰 열이 중복되어서 뽑히기 때문에 원하는 결과를 얻기가 힘들다



# 3. t-SNE : t-Stochastic Neighborhood Embedding

PCA로 차원 축소를 하면 2차원에서 명확하게 보였던 클러스터들이 1차원이 되면서 뭉개질 수 있다. 그래서 이를 해결하기 위해서 SNE 방법이 나왔다 . 

데이터 간의 거리를 구하여서 클러스터를 차원축소한다. 고차원에서 거리가 가까우면 저차원에서도 더 가깝도록 구성 / 거리가 멀면 더 멀도록 구성한다. 거리를 확률분포에 mapping.

tSNE : SNE의 단점을 보완

기존 SNE는 거리만 확률로 계산했는데, 만약에 거리가 너무 멀면 너무 멀게 표시하다보니 가깝게 표현하는 애들은 보이지도 않게 뭉쳐있음 => 이를 조절해주기 위해

정규분포 대신 Student-t 분포를 사용한다.



