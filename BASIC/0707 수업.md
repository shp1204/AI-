# **LECTURE01_회귀**

## 1. 회귀 개념 알아보기

- 어떻게 찾을까 ? 산 정상 오르기
- Gradient Descent 가 100% 정확한 값을 찾는 것은 아니다
- 선형회귀에서 least square 방법을 많이 사용하지만 다른 모델에서 사용하기 어렵기 때문에 Gradient Descent를 많이 사용함

## 2. 단순 선형회귀

![image-20200707141957630](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707141957630.png)



ex ) 평균 기온에 따른 아이스크림 판매량을 예측해보자

## 3. 다중 선형회귀

ex ) 평균기온, 평균 강수량에 따른 아이스크림 판매량을 예측해보자

![image-20200707142552355](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707142552355.png)



![image-20200707142149923](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707142149923.png)

* 평균 기온과 평균 강수량 사이에도 어떤 관계가 있다면 ? 위의 모델을 사용하기 어렵다.



## 다항회귀

* 위의 모델을 더 알맞게 fitting할 수 없을까 ? => 다항회귀

  ![image-20200707142627259](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707142627259.png)

  ![image-20200707142657140](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707142657140.png)

  ![image-20200707142743632](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707142743632.png)

## 5. 과적합과 정규화

다항 회귀 모델로 예측을 했더니 ( 제곱값을 사용하기 때문에 ) 과적합이 발생한다 

=> 오차가 커짐을 의미한다( test 데이터에서 성능이 안좋음 )

=> 교차검증과 정규화로 이 문제를 해결하자



1 ) 정규화 : 모델의 복잡성을 줄인다

= 패널티를 부여한다 ?? 이게 무슨말일까?

```
y = ax^2 + bx + c 로 곡선의 그래프를 그릴 수 있다

이 때 a값을 엄청 줄이면 ax^2값이 매우 작기 때문에 y는 직선과 비슷하게 그려진다

L1, L2 방식으로 위의 과정을 수행할 수 있다
* L1 : Lasso = 불필요한 입력값에 대응되는 베타값을 정확히 0으로 만든다
* L2 : Ridge = 아주 큰 값이나 작은 값을 가지는 이상치에 대한 베타값을 0에 가까운 값으로 만든다
* 엘라스틱 넷
```

2 ) 교차검증 : 훈련용 데이터와 별개의 테스트 데이터, 검증 데이터로 나누어 성능을 평가하는 방법이다

가장 많이 사용하는 방법 : k-fold 교차 검증

![image-20200707143522085](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707143522085.png)

validation data를 만듬으로써 학습중에도 평가를 진행한다

=> 학습 중에 과적합이 되지 않을지 확인하는데 k번 만큼 확인한다



## 6. 회귀 알고리즘 평가지표

1 ) RSS : 단순 오차 제곱합

ERROR가 많을수록 RSS값이 커진다. 

![image-20200707143906759](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707143906759.png)

* Scaling 전 후의 오차가 많이 차이나면 RSS로는 비교가 불가능하다

  2 ) MSE : 평균제곱오차

  3 ) MAE : 평균절대값 오차

![image-20200707144117455](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707144117455.png)

* 얘도 Scaling문제를 해결할 수 없ㄷㅏ

  4 ) R2

  ```
  R2 = 1 - RSS / Total Variance
  (0 <= R2 <= 1 )
  ```

  ![image-20200707144352346](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707144352346.png)

  

# LECTURE02_분류

## 1. 분류 개념과 로지스틱 회귀

ex ) 과거 기상 정보와 그에 따른 항공 지연 여부

![image-20200707150430277](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707150430277.png)

1 ) 선형 회귀로 그어보자

![image-20200707150459825](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707150459825.png)

=> 점들을 잘 설명할 수 없다

=> 로지스티 회귀로 그어보자

2 ) 로지스틱 회귀 : 이진 분류에 적합하다

![image-20200707150532968](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707150532968.png)

이 함수의 형태를 시그모이드 라고 함.

![image-20200707150647075](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707150647075.png)

0.5 왼쪽은 모두 0, 오른쪽은 1로 본다고하자.

0.5를 경계 기준으로 삼는다.



## 2. SVM(Support Vector Machine)

딥러닝 암흑기에 유용했음

최적화가 깔끔함

![image-20200707150936419](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707150936419.png)

이 두가지를 잘 나누고 싶다

어떻게 잘 나눌수 있을까?

방법 :  데이터 군으로부터 최대한 멀리 떨어지는 것으로 부터 선을 긋는 방식

 ![image-20200707151059147](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707151059147.png)

![image-20200707151124708](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707151124708.png)

support vector와 결정 경계 사이를 margin이라고 하는데 이를 최소로 하는 결정 경계의 a, b 값을 찾는다

1 ) Hard Margin

![image-20200707151320488](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707151320488.png)

2 ) Soft Margin

![image-20200707151328750](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707151328750.png)

soft margin은 결정경계가 있음에도 불구하고 margin 내의 값을 허용한다 -> 이상치라고 생각

=> 모델이 더 복잡함

svm 의 장점 : sotf margin을 허용하면서 최적화시킬 수 있으므로 많이 사용

![image-20200707151510911](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707151510911.png)

## 3. 나이브 베이즈 분류



![image-20200707151626089](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707151626089.png)

ex ) 맑은 날 비가 오지 않을 확률 ?

![image-20200707151758524](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707151758524.png)

ex ) 스팸 메일과 정상 메일 분류

![image-20200707151909944](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707151909944.png)

![image-20200707152119236](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707152119236.png)

## 4. KNN

![image-20200707152253833](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707152253833.png)

새로운 검은색 점이 들어왔을 때

--------------------------------------------

가장 가까운 한 점으로 판단하면 -> 주황색

세개로 가정한다면 -> 연두색

----

K 값을 결정하는 것이 중요함.

![image-20200707152419822](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707152419822.png)

## 5. 분류 알고리즘 평가 지표

### 1 ) 정확도

![image-20200707152518422](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707152518422.png)

![image-20200707152529985](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707152529985.png)

ex ) 전체 100개 항공기 지연여부 예측

![image-20200707152641384](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707152641384.png)

![image-20200707152654671](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707152654671.png)

![image-20200707152706375](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707152706375.png)

하지만, 불균형한 클래스에서의 정확도만으로 평가할 수 없다

따라서 다른 평가지표 필요

### 2 ) 정밀도

### 3 ) 재현율

### 4 ) FPR

### 5 ) ROC Curve, AUC



# **LECTURE03_비지도학습**

## 1. 비지도학습

![image-20200707154304279](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707154304279.png)

## 2. 클러스터링

ex ) 고객별 구매 상품 개수 데이터를 활용하여 유사한 고객 집단으로 세분화하고자 한다면 ? 

![image-20200707154509843](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707154509843.png)

![image-20200707154622831](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707154622831.png)

![image-20200707154635756](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707154635756.png)

여러 군데에 속할 수 있는 것이 sotf clustering이다.

[군집 간 클러스터링]

![image-20200707154703473](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707154703473.png)

## 3. k-means Clustering

![image-20200707154903624](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707154903624.png)

k를 몇개로 설정하느냐에 따라서 군집 개수가 정해진다.

![image-20200707155121673](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707155121673.png)

변화가 없을 때 까지 중심을 옮겨가면서 군집화한다.

![image-20200707155642051](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707155642051.png)

## 4. Gaussian Mixture Model(GMM)

:1 번 군집에도 들어가고 2번 군집에도 들어갈 수 있게 만들자

![image-20200707155847275](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707155847275.png)

빨간색, 초록색, 보라색에 속할 확률을 각각 구함

![image-20200707160054606](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707160054606.png)

개별 데이터가 어떤 정규분포에 속하는지 결정

평균과 분산을 알았을 때 확률값이 어느정도인지 예상할 수 있다

각각의 점들에 대해서 확률값으로 표현 가능하다

* k-means vs gmm

k-means의 중심 파악 - > 개별데이터가 어떤 군집에 속하는지 확률로 구해서 넣어줌

![image-20200707160324881](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707160324881.png)

1 ) 클러스터 확률, 평균, 분산을 랜덤하게 초기화

2 ) 변화가 없을 때 까지 어느 클러스터에 들어가는지 반복 계산

![image-20200707160530071](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707160530071.png)

## 5. 차원 축소

고차원 데이터를 저차원으로 줄이는 알고리즘

## 6. 주성분 분석 (PCA)

![image-20200707160914207](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707160914207.png)

데이터로부터 직선으로 선을 그어본다.

![image-20200707160953611](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707160953611.png)

다른 선을 그어본다. 파란선은 저보들이 완전 소실된다. 따라서 파란 선을 지양하고 초록 선을 잘 사용해보고자 한다.

![image-20200707161158301](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707161158301.png)

![image-20200707161205872](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707161205872.png)

## 7. t-SNE

고차원 공간에서 차원을 축소하는 방법

![image-20200707161405946](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707161405946.png)

데이터 분포표를 만든다. 1번데이터 ~ N번 데이터 각각 거리에 따른 확률을 구한다.

![image-20200707161456712](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707161456712.png)

처음에 이를 축소하면

![image-20200707161509175](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707161509175.png)

![image-20200707161525940](C:\Users\박소희\Desktop\AI College\BASIC\0707 수업.assets\image-20200707161525940.png)

점들 간의 거리가 있었는데 이 거리를 활용하여 점을 옮겨준다.

![image-20200707161535585](0707%20%EC%88%98%EC%97%85.assets/image-20200707161535585.png)

![image-20200707161653798](0707%20%EC%88%98%EC%97%85.assets/image-20200707161653798.png)



# **LECTURE04_의사결정나무**

## 1. 의사결정나무

## 2. 의사결정나무 회귀

![image-20200707163638882](0707%20%EC%88%98%EC%97%85.assets/image-20200707163638882.png)

어떤 구역으로 나누어야할까 ? RSS : 오차들의 합으로 최소한의 구역으로 만들자. 

단순한데 성능이 좋다

## 3. 분류

카테고리들이 나오는데 YES, NO 경우에 터미널이 많이 생성된다.

RSS를 사용하지 않고 데이터의 불순도를 사용한다. A와 B 중 불순도가 더 낮은지에 대해 계산하는 방식(지니계수, 엔트로피)으로 판별한다.

 ![image-20200707164123001](0707%20%EC%88%98%EC%97%85.assets/image-20200707164123001.png)

![image-20200707164250588](0707%20%EC%88%98%EC%97%85.assets/image-20200707164250588.png)



## 4. 앙상블

의사결정나무를 하나만 사용하지 말고 여러개를 사용해보자