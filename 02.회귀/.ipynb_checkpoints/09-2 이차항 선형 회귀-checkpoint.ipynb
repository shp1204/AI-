{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2장. 다항 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 목표\n",
    "- 다항 선형 회귀 모델의 구조와 고차항이 추가될수록 변화하는 특성을 학습합니다. \n",
    "- 다항 선형 회귀 class를 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 다항 선형 회귀 모델\n",
    "1. 다항 선형 회귀란?\n",
    "\n",
    "### 2. 이차항 선형 회귀\n",
    "1. 이차항 선형 회귀 모델\n",
    "2. 이차항 선형 회귀 class 구조\n",
    "3. scikit-learn 모듈\n",
    "\n",
    "### 3. 고차항 선형 회귀\n",
    "1. 고차항 선형 회귀 모델\n",
    "2. 고차항 선형 회귀 class 구조\n",
    "3. scikit-learn 모듈\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  이차항 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 이차항 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이차항 선형 회귀 모델을 정의하기 전에 데이터부터 정의해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 학습용 feature 데이터\n",
    "\n",
    "> $$\\mathbf{x_{feature}}=\\begin{pmatrix}\n",
    " x_{1} \\\\ \n",
    " x_{2} \\\\ \n",
    " \\vdots  \\\\ \n",
    " x_{N} \n",
    "\\end{pmatrix}, \\;\\;\\;\\;\\; $$\n",
    "\n",
    "##### 학습용 lable 데이터\n",
    "\n",
    "> $$\\mathbf{y}=\\begin{pmatrix}\n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "\\vdots \\\\ \n",
    "y_N\n",
    "\\end{pmatrix}. \\;\\;\\;\\;\\; $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음부터 $p$개의 일반화 된 feature 데이터를 정의를 사용하여 식을 전개하기엔 쉽지 않기에, 우선 한 종류의 feature column을 갖는 feature 데이터로 시작해서 고차항 파트에서 일반화 해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이차항 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이차항 선형 회귀 모델에 대해서 정의해 봅시다.\n",
    "\n",
    "이차항 선형 회귀 모델은 1개의 feature column을 입력으로 하여 이차 함수 형태로 회귀 모델을 구현한 것을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 이차항 선형 회귀 모델\n",
    "\n",
    "> $$f(x_i)=w_0+w_1 x_{i}+w_2 x_{i}^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_0, w_1, w_2$은 1차 함수 모델 $f()$의 파라미터를 의미합니다.\n",
    "\n",
    "다중 선형 회귀에서처럼 least square solution을 구하기 위해서 보다 효율적으로 행렬식으로 변환해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 이차항 선형 회귀 모델\n",
    "\n",
    "> $$F(X) = X\\mathbf{w}$$\n",
    "\n",
    "> $$X=\\begin{pmatrix}\n",
    "1 & x_{1} & x_{1}^{2} \\\\ \n",
    "1 & x_{2} & x_{2}^{2} \\\\ \n",
    "\\vdots & \\vdots  & \\vdots \\\\ \n",
    "1 & x_{N} & x_{N}^{2}\n",
    "\\end{pmatrix}, \\;\\;\\;\\;\\; \\mathbf{w}=\\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\ \n",
    "w_2 \\\\ \n",
    "\\end{pmatrix}, \\;\\;\\;\\;\\; $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$는 기존 feature 데이터에서 1만으로 이루어진 column과 $x_{i}^{2}$ 데이터의 column을 추가한 형태를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 1> 이차항 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이차항 선형 회귀 모델을 만들어 feature 데이터를 입력하고 예측값을 출력하여 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_feature: \n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "\n",
      "X: \n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  2.  4.]\n",
      " [ 1.  3.  9.]\n",
      " [ 1.  4. 16.]]\n",
      "\n",
      "Y: \n",
      "[[3.1]\n",
      " [4.9]\n",
      " [7.2]\n",
      " [8.9]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# feature 데이터\n",
    "X_feature = np.array([1, 2, 3, 4]).reshape((-1,1))\n",
    "# 1 column 이 추가된 feature 데이터\n",
    "X = np.c_[np.ones((X_feature.shape[0],1)),X_feature]\n",
    "# 제곱 데이터 column이 추가된 feature 데이터\n",
    "X = np.c_[X,X_feature**2]\n",
    "# label 데이터\n",
    "Y = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))\n",
    "\n",
    "print(\"X_feature: \\n{}\\n\".format(X_feature))\n",
    "print(\"X: \\n{}\\n\".format(X))\n",
    "print(\"Y: \\n{}\\n\".format(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F(X): \n",
      "[[ 3.]\n",
      " [ 7.]\n",
      " [13.]\n",
      " [21.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 column 설정\n",
    "w = np.array([1,1,1]).reshape((-1,1))\n",
    "\n",
    "# 다항 선형 모델 함수\n",
    "def F_X(w,X):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "print(\"F(X): \\n{}\\n\".format(F_X(w,X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss 함수 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다항 선형 회귀 모델을 정리 했으니 마지막으로 loss 함수를 정리해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss 함수\n",
    "\n",
    "> $$\n",
    "\\begin{aligned}\n",
    "Loss(\\mathbf{w})&=\\frac{1}{N}\\sum_{i=1}^{N}(y_i-f(\\mathbf{x_i}))^{2} \\\\\n",
    "&=\\frac{1}{N}(\\mathbf{y}-X\\mathbf{w})^{T}(\\mathbf{y}-X\\mathbf{w})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 수식에서 알 수 있듯이, 다중 선형 회귀와 형태는 같습니다. 그저 2 번째 feature 데이터의 column이 첫 번째 column의 제곱이란 것만 다를뿐입니다. \n",
    "\n",
    "따라서 같은 방식으로 least square solution을 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 이차항 선형 회귀 파라미터 least square solution\n",
    "\n",
    "> $$\\mathbf{\\widehat{w}}=(X^{T}X)^{-1}X^{T}\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\widehat{w}}$은 loss 값을 최소로 만드는 파라미터 벡터를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 2> 이차항 선형 회귀 구현 - least square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이차항 선형 회귀 최적의 파라미터를 least square로 구하고 최소화 된 loss 값을 출력해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.015124999999999977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# feature 데이터\n",
    "X_feature = np.array([1, 2, 3, 4]).reshape((-1,1))\n",
    "# 1 column 이 추가된 feature 데이터\n",
    "X = np.c_[np.ones((X_feature.shape[0],1)),X_feature]\n",
    "# 제곱 데이터 column이 추가된 feature 데이터\n",
    "X = np.c_[X,X_feature**2]\n",
    "# label 데이터\n",
    "Y = np.array([3.1, 4.9, 7.2, 8.9]).reshape((-1,1))\n",
    "\n",
    "\n",
    "w = np.dot(np.linalg.inv(np.dot(X.transpose(),X)),np.dot(X.transpose(),Y))\n",
    "\n",
    "# 다중 선형 모델 함수\n",
    "def F_X(w,X):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "\n",
    "def loss(f_x, label_data):\n",
    "    error = label_data - f_x\n",
    "    ls = np.mean(error**2)\n",
    "    return ls\n",
    "\n",
    "print(\"loss: {}\\n\".format(loss(F_X(w,X),Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 이차항 선형 회귀 class 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 수행된 것을 확인해보면 이차항 선형 회귀의 class는 $X$를 만들어주는 것 이외에는 다중 선형 회귀의 class와 다를게 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 3> 이차항 선형 회귀 class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class는 least square를 사용했던 단순 선형 회귀 구조와 비슷하게 초기화, 학습, 예측, loss 함수로 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class sec_linaer_regression:\n",
    "    # 초기화 함수\n",
    "    def __init__(self, initial_w):\n",
    "        self.w = initial_w\n",
    "        \n",
    "    # 학습 함수    \n",
    "    def fit(self, feature, label):\n",
    "        \n",
    "        # 이차항을 추가하여 X를 만드는 과정\n",
    "        X = np.c_[np.ones((feature.shape[0],1)),feature]\n",
    "        X = np.c_[X,feature**2]\n",
    "        self.w = np.dot(np.linalg.inv(np.dot(X.transpose(),X)),np.dot(X.transpose(),label))\n",
    "\n",
    "        \n",
    "    # 예측값 계산 함수\n",
    "    def predict(self, feature):\n",
    "        \n",
    "        # 이차항을 추가하여 X를 만드는 과정\n",
    "        X = np.c_[np.ones((feature.shape[0],1)),feature]\n",
    "        X = np.c_[X,feature**2]\n",
    "        prediction = np.dot(X,self.w)\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    # loss 값 계산 함수\n",
    "    def loss(self, feature, label):\n",
    "        \n",
    "        error = label - self.predict(feature)\n",
    "        ls = np.mean(error**2)\n",
    "        \n",
    "        return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 4> 이차항 선형 회귀 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<예제 3>` 이차항 선형 회귀 class에서 구현한 class를 사용하여 학습 과정을 수행해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.015124999999999977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 학습용 데이터\n",
    "feature_data = np.array([1, 2, 3, 4]).reshape((-1,1))\n",
    "label_data = np.array([3.1,4.9,7.2,8.9]).reshape((-1,1))\n",
    "\n",
    "# 파라미터 0으로 초기화\n",
    "w = np.zeros((feature_data.shape[1]+1,1))\n",
    "            \n",
    "# 다중 선형 회귀 모델 불러오기 및 초기화\n",
    "model = sec_linaer_regression(w)\n",
    "\n",
    "# 학습 수행\n",
    "model.fit(feature_data, label_data)\n",
    "\n",
    "print(\"loss: {}\\n\".format(model.loss(feature_data, label_data))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. scikit-learn 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn에서는 이차항 선형 회귀 또한 class가 따로 존재 하지 않습니다. \n",
    "\n",
    "역시나 단순 선형 회귀에서 사용한 `LinearRegression()` class는 이차항 선형 회귀에서도 그대로 사용됩니다.\n",
    "\n",
    "다만 입력하는 feature를 $X$로 만드는 과정이 없기에 `PolynomialFeatures` 을 사용하여 $X$를 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 5> `PolynomialFeatures` 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이차항 선형 회귀를 scikit-learn으로 수행하기 위해서 `PolynomialFeatures`을 사용하여 feature 데이터의 이차항 column 배열을 추가해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature data: \n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "\n",
      "X: \n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  2.  4.]\n",
      " [ 1.  3.  9.]\n",
      " [ 1.  4. 16.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "feature_data = np.array([1, 2, 3, 4]).reshape((-1,1))\n",
    "print(\"feature data: \\n{}\\n\".format(feature_data))\n",
    "\n",
    "# 2차항 선형 회귀에 해당되는 2를 초기값으로 입력\n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "# fit 함수를 사용하여 X로 변환 작업을 수행\n",
    "X = poly.fit_transform(feature_data)\n",
    "print(\"X: \\n{}\\n\".format(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PolynomialFeatures`을 사용하여 $X$가 준비되었다면 단순 선형 회귀와 같은 방법으로 학습을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <예제 6>  scikit-learn을 사용한 이차항 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<예제 4>` 에서와 같은 데이터를 사용하여 이번엔 scikit-learn의 loss 값을 출력해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.015125000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 모델 설정\n",
    "sci_model = LinearRegression()\n",
    "\n",
    "# 학습 수행\n",
    "sci_model.fit(X, label_data)\n",
    "\n",
    "# scikit-learn 에서는 loss 함수가 모델안에 내정되어 있지 않기에 정의\n",
    "def loss(prediction, label):\n",
    "        \n",
    "    error = label - prediction\n",
    "    ls = np.mean(error**2)\n",
    "\n",
    "    return ls\n",
    "\n",
    "print(\"loss: {}\\n\".format(loss(sci_model.predict(X), label_data))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
