{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 크롤링\n",
    "크롤링 : 웹페이지에서 필요한 데이터를 추출해내는 작업\n",
    "\n",
    "웹 페이지는 정보를 HTML문서로 표현한다.\n",
    "\n",
    "웹 페이지의 HTML을 얻기 위해 requests 라이브러리를 사용하고, 가져온 HTML을 분석하기 위해 BeaurifulSoup 라이브러리를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BeautifulSoup 라이브러리\n",
    "\n",
    "HTML 파일로 BeautifulSoup 객체를 만들고, 관습적으로 이름을 Soup이라고 한다.\n",
    "\n",
    "html.parser : BeautifulSoup 객체에게 'HTML을 분석해라'\n",
    "\n",
    "find, find_all 메소드를 이용하여 HTML 태그를 추출할 수 있다\n",
    "- find : 태그 하나\n",
    "- find_all : 모든 태그를 담은 리스트\n",
    "\n",
    "get_text : 태그가 갖고 있는 텍스트를 얻을 수 있다\n",
    "\n",
    "id 매개변수 값을 지정해서 사용할 수도 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.html을 불러와서 beautifulsoup 객체를 초기화해 soup에 저장하세요.\n",
    "soup = BeautifulSoup(open('index.html'), 'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# index.html을 불러와서 beautifulsoup 객체를 초기화해 soup에 저장하세요.\n",
    "soup = BeautifulSoup(open('index.html'), 'html.parser')\n",
    "\n",
    "# soup를 사용하여 요구되는 정보를 출력해보세요.\n",
    "print(soup.find('p').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# index.html을 불러와서 beautifulsoup 객체를 초기화해 soup에 저장하세요.\n",
    "soup = BeautifulSoup(open('index.html'),'html.parser')\n",
    "\n",
    "# class = elice > div > p\n",
    "# soup를 사용하여 요구되는 정보를 출력해보세요.\n",
    "print(soup.find('div', class_ = \"elice\").find('p').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# index.html을 불러와서 beautifulsoup 객체를 초기화해 soup에 저장하세요.\n",
    "soup = BeautifulSoup(open('index.html'), 'html.parser')\n",
    "\n",
    "# soup를 사용하여 요구되는 정보를 출력해보세요.\n",
    "print(soup.find('div', id='main').find('p').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python에서 요청을 보낼 수 있는 모듈\n",
    "\n",
    "GET 요청 : 정보를 조회하기 위한 요청\n",
    "\n",
    "POST 요청 : 정보를 생성, 변경하기 위한 요청\n",
    "\n",
    "url = 'http:///~~'\n",
    "\n",
    "result = request.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.naver.com\"\n",
    "\n",
    "# url 변수에 담긴 url의 html 문서를 불러와 출력해보세요.\n",
    "req = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 네이버 헤드 뉴스 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T05:47:41.366170Z",
     "start_time": "2020-06-13T05:47:40.946870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "서울 '입원 격리' 확진자 408명 역대 최대…전국의 38% 차지\n",
      "포스코 포항제철소에 불 \"인명피해 없어\"\n",
      "북 외무성 \"비핵화 소리 집어치워야…남측, 논할 신분 안돼\"\n",
      "'코로나19 총력방역'…고글 등장한 올해 최대 공무원 공채시험\n",
      "폭우에 잇단 교통사고…농경지.주택 침수ㆍ배수구 역류\n",
      "'9살 여아 지옥학대' 계부 전격 체포…묵묵부답으로 일관\n",
      "여야, 원구성 치킨 게임…\"이미 양보\" vs \"의회독재\"\n",
      "대검 감찰부장 \"한명숙·검언유착 사건 '사심없이'\"\n",
      "효순·미선 18주기 추모제 열려…사고현장에 평화공원\n",
      "\"아시아로 꺼져\" 미국 중년여성 욕설영상 일파만파\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#yna_rolling > div\n",
    "\n",
    "def crawling(soup) :\n",
    "    # soup 객체에서 추출해야 하는 정보를 찾고 반환하세요.\n",
    "    result = soup.find('div', id='yna_rolling').get_text()\n",
    "        \n",
    "    return result\n",
    "    \n",
    "\n",
    "def main() :\n",
    "    url = \"http://www.naver.com\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    # crawling 함수의 결과를 출력합니다.\n",
    "    print(crawling(soup))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 연합뉴스 속보기사 제목 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T06:00:29.105270Z",
     "start_time": "2020-06-13T06:00:28.852091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '리치웨이발 급확산 하루 14명 늘어 153명…실내체육관까지 전파', '강남 어학원 확진자들 중랑구 헬스장 방문…\"이용자 수백명\"', \"서울 '입원 격리' 확진자 408명 역대 최대…전국의 38% 차지\", \"세계 '코로나 재유행' 골머리…버티기 장기전 계속된다\", '포스코 포항제철소에 불 \"인명피해 없어\"', '북 외무성 \"비핵화 소리 집어치워야…남측, 논할 신분 안돼\"', \"'코로나19 총력방역'…고글 등장한 올해 최대 공무원 공채시험\", '폭우에 잇단 교통사고…농경지.주택 침수ㆍ배수구 역류', \"'9살 여아 지옥학대' 계부 전격 체포…묵묵부답으로 일관\", '여야, 원구성 치킨 게임…\"이미 양보\" vs \"의회독재\"', '대검 감찰부장 \"한명숙·검언유착 사건 \\'사심없이\\'\"', '효순·미선 18주기 추모제 열려…사고현장에 평화공원', '\"아시아로 꺼져\" 미국 중년여성 욕설영상 일파만파']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup) :\n",
    "    # soup 객체에서 추출해야 하는 정보를 찾고 반환하세요.\n",
    "    div = soup.find('div', 'list_body')\n",
    "    \n",
    "    result = []\n",
    "    for a in div.find_all('a') :\n",
    "        result.append(a.get_text())\n",
    "    return result\n",
    "    \n",
    "\n",
    "def main() :\n",
    "    url = \"https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&sid1=001&sid2=140&oid=001&isYeonhapFlash=Y\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    # crawling 함수의 결과를 출력합니다.\n",
    "    print(crawling(soup))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. bugs 실시간 음원차트 순위 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T06:09:27.734403Z",
     "start_time": "2020-06-13T06:09:26.602599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MORE & MORE', '에잇(Prod.&Feat. SUGA of BTS)', '작사가', '사랑하게 될 줄 알았어', '살짝 설렜어 (Nonstop)', '아로하', 'Dolphin', '너에게 난, 나에게 넌', '깡 Official Remix', '품', '밤이 깊었네 (Drama Ver.)', '좋은 사람 있으면 소개시켜줘', '일이 너무 잘 돼', 'Blueming', '나비와 고양이 (feat. 백현 (BAEKHYUN))', '화려하지 않은 고백', '흔들리는 꽃들 속에서 네 샴푸향이 느껴진거야', 'Memories', '그대 고운 내사랑', \"Don't Start Now\", '너의 이름은 (Feat. ASH ISLAND)', 'LALALILALA', 'Dance Monkey', 'Senza Luce', 'CHECKMATE', '덤더럼(Dumhdurum)', 'Juice', 'Happy', '시작', '내 눈물 모아', 'Falling Slowly', '신세계 (New World)', 'WANNABE', '어떻게 지내 (Prod. By VAN.C)', 'METEOR', 'Lonely Night', 'Stuck with U', '12:45 (Stripped)', 'Maniac', '아무노래', '자나깨나 (Feat. 조이 of Red Velvet)', '니가 왜 거기서 나와 (Narr. 고은아)', 'Square (2017)', 'Psycho', '2002', 'FIESTA', '아 진짜요. (Oh really.)', 'Zombie', 'BUTTERFLY', '처음처럼', 'Blinding Lights', 'Stay Tonight', '어떻게 지내 (답가)', 'Painkiller', '살아가는 거야', '깡', '모든 날, 모든 순간 (Every day, Every Moment)', '도원경 (Quasi una fantasia)', 'Be Yourself', '시청 앞 지하철 역에서', 'Say Yes (Feat.\\xa0문별\\xa0of\\xa0마마무)', '찐이야', '마음을 드려요', '어떻게 이별까지 사랑하겠어, 널 사랑하는 거지', '작은 것들을 위한 시 (Boy With Luv) (Feat. Halsey)', '넌 언제나', 'To Die For', 'BUNGEE (Fall in Love)', 'Te Quiero, Te Quiero', '사랑하게 될 거야 (We Must Love)', '돌덩이', '주라주라', 'REVEAL (Catching Fire)', 'Birthday', 'Candy', '늦은 밤 너의 집 앞 골목길에서', 'Rain On Me', '그때 그 아인', '밤편지', 'HIP', '사랑은 지날수록 더욱 선명하게 남아', 'Say So', '막걸리 한잔', 'Fadeaway', 'bad guy', '이제 나만 믿어요', '주저하는 연인들을 위해', 'Paris In The Rain', 'Complete (널 만난 순간)', '오늘도 빛나는 너에게 (To You My Light) (Feat.이라온)', 'Love poem', 'Oh my god', 'Love me or Leave me', \"It's Raining\", '우리 만남이', '너를 만나', '사계 (Four Seasons)', 'Physical (feat. 화사)', '봄날에 물드는 것', 'The 사랑하게 될 거야']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup) :\n",
    "    # soup 객체에서 추출해야 하는 정보를 찾고 반환하세요.\n",
    "    div = soup.find('tbody').find_all('p',class_='title')\n",
    "\n",
    "    result = []\n",
    "    for a in div :\n",
    "        text = a.find('a').get_text().replace(\"\\n\",\"\")\n",
    "        result.append(text) \n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def main() :\n",
    "    url = \"https://music.bugs.co.kr/chart\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    # crawling 함수의 결과를 출력합니다.\n",
    "    print(crawling(soup))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 영화 후기 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T06:20:14.149206Z",
     "start_time": "2020-06-13T06:20:13.829980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['다들 구라치지마세요 재미1도 없는 영화', '닐 암스트롱 vs 스티븐 시걸', '미국의 성공과 암스트롱의 성공, 그 이면의 이야기', '진짜 영화 제대로 보지도않고, 볼줄도 모르는 사람이 안타깝다', '[영화] 퍼스트맨 (2018), \"우주보단 인간, 노래보단 적막, 라라랜드보단 위플래쉬\"', '\"적막을 감성과 감정으로 채운 영화\" 퍼스트맨 짧은리뷰 스포없음', '데미언 셔젤은 왜 ‘닐 암스트롱’을 선택했는가 (in 2018 부산국제영화제)', '<퍼스트맨(First Man , 2018)> 달에 첫 발을 내딛기까지.. 닐 암스트롱의 전기 영화 (데이미언 셔젤 감독, 라이언 고슬링, 클레어 포이, 실화 영화)', '천재감독의 3번째 영화!!!', '정말 많은걸 느끼게 한듯']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup) :\n",
    "    # soup 객체에서 추출해야 하는 정보를 찾고 반환하세요..\n",
    "    ul = soup.find('ul', class_ = 'rvw_list_area').find_all('li')\n",
    "\n",
    "    \n",
    "    result = []\n",
    "    for a in ul :\n",
    "        result.append(a.find('a').get_text())\n",
    "    return result\n",
    "\n",
    "\n",
    "def main() :\n",
    "    url = \"https://movie.naver.com/movie/bi/mi/review.nhn?code=168058#\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    # crawling 함수의 결과를 출력합니다.\n",
    "    print(crawling(soup))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 커뮤니티 댓글 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T06:46:57.951010Z",
     "start_time": "2020-06-13T06:46:57.043365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 우리 댕댕이ㅋ', '우리집도 말티 댕댕이들은 사랑입니당!', 'ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 다이쁨 완전 얼짱이넹', '우리 딸랑이', '진짜 이쁘네여 ㅎㅎㅎㅎㅎ', '우리집도 말티즈ㅠㅠㅠ', '우리친구 자기주장이 무척 강하게 생겼네요', '말티는 사랑입니당^^', '오우 ses 바다 가 생각나는 헤어사타일이군요', '너무 귀엽당^^', '밑에서 두번째 사진 털때문인지 뾰루퉁한거 넘 귀여워요 ㅋㅋㅋㅋㅋ!울집강아지도 털때문에 가끔 눈이 화난눈 되거든요 ㅎㅎㅎㅎ 귀엽 ㅠ.ㅠ*', '우리집도', '달릴 때 졸귘ㅋㅋ 눈이랑 코랑 동글동글 너무 귀엽 ㅠㅠㅠㅠㅠ', '안녕 칭구', '개똥냄새 쩔게 생겼네', '설탕 아니고 소금이', '안녕 난 두부야', '아 너모 이쁘다. 항상 건강하자 아가', '힐링하고 갑니다❤️', '안뇽 ?', '네츄럴 부스스.. 예쁘네요..^^']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawling(soup) :\n",
    "    # soup 객체에서 추출해야 하는 정보를 찾고 반환하세요.\n",
    "    dd = soup.find_all('dd', class_ = 'usertxt')\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for dl in dd :\n",
    "        text = dl.get_text().replace(\"\\t\", \"\").replace(\"\\n\", \"\")\n",
    "        result.append(text)\n",
    " \n",
    "    return result\n",
    "    \n",
    "\n",
    "\n",
    "def main() :\n",
    "    url = \"https://pann.nate.com/talk/350939697\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    # crawling 함수의 결과를 출력합니다.\n",
    "    print(crawling(soup))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
